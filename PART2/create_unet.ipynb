{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/June103310110/Image_Segmentation/blob/main/PART2/create_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c809ac",
      "metadata": {
        "id": "58c809ac"
      },
      "source": [
        "# Unet\n",
        "source: https://amaarora.github.io/2020/09/13/unet.html\n",
        "\n",
        "<img src=\"https://i.imgur.com/LQORH9i.png\" alt=\"drawing\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d85f06b",
      "metadata": {
        "id": "8d85f06b"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_LABELS = 1\n",
        "WIDTH = 512\n",
        "HEIGHT = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60bd557",
      "metadata": {
        "id": "f60bd557"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch # 1.9\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "732de5f6",
      "metadata": {
        "id": "732de5f6"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb648dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eb648dc",
        "outputId": "ce8f3d8a-c15b-4f80-8a42-a464c25618b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cpu' # for debug建議使用cpu作為torch的運行背景\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9876be87",
      "metadata": {
        "id": "9876be87"
      },
      "source": [
        "## ConvBlock\n",
        "- 加入Instance Norm.\n",
        "- <img src=\"https://miro.medium.com/max/983/1*p84Hsn4-e60_nZPllkxGZQ.png\" width=\"50%\">\n",
        "\n",
        "> 上圖為一整個batch的feature-map。輸入6張圖片，輸入6chs, 輸出也是6chs(C方向看進去是channel, N方向看進去是圖片)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 原始版本\n",
        "原始版本是單純的Stack convolution"
      ],
      "metadata": {
        "id": "4YJ5tl9kscWc"
      },
      "id": "4YJ5tl9kscWc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4a1178",
      "metadata": {
        "id": "3b4a1178"
      },
      "outputs": [],
      "source": [
        "# # 原始版本\n",
        "# class convBlock(nn.Module):\n",
        "#   def __init__(self, in_ch, out_ch):\n",
        "#     super().__init__()\n",
        "#     self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "#     self.relu  = nn.ReLU()\n",
        "#     self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     return self.relu(self.conv2(self.relu(self.conv1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 修改版本\n",
        "- 參考1: Residual Unet [source](https://arxiv.org/abs/1711.10684)\n",
        "- 參考2: 使用Instance Normalization。讓輸出的各個feature-map更robust。"
      ],
      "metadata": {
        "id": "ktp8Sb46sjrV"
      },
      "id": "ktp8Sb46sjrV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acecaf5b",
      "metadata": {
        "id": "acecaf5b"
      },
      "outputs": [],
      "source": [
        "# 參考2\n",
        "## 加入instance normalization and no residual\n",
        "class convBlock(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch, padding = 'same', kernel_size=3):\n",
        "    super().__init__()\n",
        "    kernel_size = kernel_size\n",
        "    pad_size = lambda kernel_size:(kernel_size-1)//2\n",
        "    if padding=='same':\n",
        "      self.padding = pad_size(kernel_size)\n",
        "    else:\n",
        "      self.padding = padding\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, padding=self.padding, bias=False)\n",
        "    self.relu  = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, padding=self.padding, bias=False)\n",
        "    self.INorm = torch.nn.InstanceNorm2d(out_ch, affine=True)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    x = self.INorm(self.conv1(x))\n",
        "    x = self.relu(x)\n",
        "    x = self.INorm(self.conv2(x))\n",
        "    x = self.relu(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "573d4081",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573d4081",
        "outputId": "93c292be-b3ae-47af-ecd2-8ef415d0a6ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 512, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "block = convBlock(1, 64)\n",
        "x = torch.randn(1, 1, WIDTH, HEIGHT)\n",
        "block(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f88b80",
      "metadata": {
        "id": "40f88b80"
      },
      "source": [
        "## Encoder(DownStream)\n",
        "將影像進行編碼，過程中解析度會縮小(maxpooling、convolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2588a584",
      "metadata": {
        "id": "2588a584"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, chs=(3,32,64,128,256,512), padding='same'):\n",
        "    super().__init__()\n",
        "    self.FPN_enc_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding) for i in range(len(chs)-1)])\n",
        "    # self.pool = nn.MaxPool2d(2)\n",
        "    self.pool = torch.max_pool2d\n",
        "      \n",
        "  def forward(self, x):\n",
        "    features = []\n",
        "    \n",
        "    for block in self.FPN_enc_ftrs:\n",
        "      x = block(x)\n",
        "      features.append(x)\n",
        "      # print(x.shape)\n",
        "      x = self.pool(x, kernel_size=2)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95ec0a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c95ec0a3",
        "outputId": "5bcbaf78-691f-40b4-b3d9-a04d2238ded0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 512, 512])\n",
            "torch.Size([1, 64, 256, 256])\n",
            "torch.Size([1, 128, 128, 128])\n",
            "torch.Size([1, 256, 64, 64])\n",
            "torch.Size([1, 512, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder()\n",
        "x = torch.randn(1, 3, WIDTH, HEIGHT)\n",
        "features = encoder(x)\n",
        "for f in features:\n",
        "  print(f.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7a435c",
      "metadata": {
        "id": "bb7a435c"
      },
      "source": [
        "## Decoder(UpStream)\n",
        "將編碼還原成影像，過程中解析度會放大直到回復成輸入影像解析度(transposed Convolution)。\n",
        "- 將編碼還原成影像是因為影像分割是pixel-wise的精度進行預測，解析度被還原後，就可以知道指定pixel位置所對應的類別\n",
        "- 類別資訊通常用feature-map的channels(chs)去劃分，一個channel代表一個class\n",
        "- 有許多UNet模型架構會有輸入576x576，但輸出只有388x388的情況，是因為他們沒有對卷積過程做padding，導致解析度自然下降。最後只要把mask resize到388x388就能繼續計算loss。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19bd7912",
      "metadata": {
        "id": "19bd7912"
      },
      "source": [
        "### Transposed Conv and UpsampleConv\n",
        "<img src=\"https://i.imgur.com/eIIJxre.png\" alt=\"drawing\" width=\"300\"/>\n",
        "<img src=\"https://i.imgur.com/uLo7icF.png\" alt=\"drawing\" width=\"300\"/>\n",
        "\n",
        "Transposed Conv \n",
        "- 透過上面的操作做轉置卷積，feature-map上的數值會作為常數與kernel相乘\n",
        "- 會導致Gridding Effect(棋盤格效應)\n",
        "\n",
        "UpsampleConv\n",
        "- 先做上採樣(Upsample/ Unpooling)\n",
        "- 然後作卷積(padding = same)\n",
        "<!-- #### 替代方案 UpSampling(Unpooling)+Convolution -->\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c6cc72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7c6cc72",
        "outputId": "45175143-ac46-43eb-ebd3-7f44ad7dfb86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 30, 56, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# ConvTranspose2d透過設定k=2, s=2, output_padding=0可以讓影像從28x28變成56x56\n",
        "\n",
        "x = torch.randn(1, 3, 28, 28)\n",
        "x = nn.ConvTranspose2d(3, 30, kernel_size=2, stride=2, output_padding=0)(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90fef2f8",
      "metadata": {
        "id": "90fef2f8"
      },
      "outputs": [],
      "source": [
        "class UpSampleConvs(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
        "    self.relu  = nn.ReLU()\n",
        "    self.upSample = nn.Upsample(scale_factor=2)\n",
        "    self.INorm = torch.nn.InstanceNorm2d(out_ch)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    x = self.upSample(x)\n",
        "    x = self.conv(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.INorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8a489a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e8a489a",
        "outputId": "3634edf3-b253-47af-a3d3-106381697c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 30, 56, 56])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(1, 3, 28, 28)\n",
        "x = UpSampleConvs(3,30)(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569d4b4c",
      "metadata": {
        "id": "569d4b4c"
      },
      "source": [
        "### decoder(上採樣) module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2b82e6",
      "metadata": {
        "id": "4f2b82e6"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs=(512, 256, 128, 64, 32), padding='same'):\n",
        "      super().__init__()\n",
        "\n",
        "      self.chs = chs\n",
        "      self.padding = padding\n",
        "      # self.upconvs = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])  # 轉置卷積\n",
        "      self.upconvs = nn.ModuleList([UpSampleConvs(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # 上採樣後卷積\n",
        "      self.FPN_dec_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding=padding) for i in range(len(chs)-1)]) \n",
        "        \n",
        "    def forward(self, x, encoder_features):\n",
        "      for i in range(len(self.chs)-1):\n",
        "        enc_ftrs = encoder_features[i]\n",
        "        \n",
        "        x = self.upconvs[i](x)\n",
        "            \n",
        "#             if self.padding == 0:\n",
        "        enc_ftrs = self.crop(encoder_features[i], x)\n",
        "        x = torch.cat([x, enc_ftrs], dim=1)\n",
        "        x = self.FPN_dec_ftrs[i](x)\n",
        "      return x\n",
        "    \n",
        "    def crop(self, enc_ftrs, x):\n",
        "      \n",
        "      _, _, H, W = x.shape\n",
        "      enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
        "      return enc_ftrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9518ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9518ce",
        "outputId": "5829afa0-d2a9-4321-ef29-96e2b92eb310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 512, 512])\n",
            "torch.Size([1, 64, 256, 256])\n",
            "torch.Size([1, 128, 128, 128])\n",
            "torch.Size([1, 256, 64, 64])\n",
            "torch.Size([1, 512, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "for i in features:\n",
        "  print(i.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9a96e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9a96e7",
        "outputId": "7da6cd56-fed7-4976-d533-738a6428101e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder()\n",
        "decoder\n",
        "x = torch.randn(1, 512, WIDTH//16, HEIGHT//16)\n",
        "print(decoder(x, features[::-1][1:]).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e74c62d",
      "metadata": {
        "id": "8e74c62d"
      },
      "source": [
        "## Unet構建\n",
        "結合encoder和decoder組成Unet。\n",
        "- 在輸出層如果用softmax做多元分類問題預測的話，類別數量要+1(num_classes+background)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95e0cc61",
      "metadata": {
        "id": "95e0cc61"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, out_sz, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, padding='same', activation=None):\n",
        "    super().__init__()\n",
        "    self.encoder     = Encoder(enc_chs, padding=padding)\n",
        "    self.decoder     = Decoder(dec_chs, padding=padding)\n",
        "    self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "    self.retain_dim  = retain_dim\n",
        "    self.out_sz = out_sz\n",
        "    self.activation = activation\n",
        "\n",
        "  def forward(self, x):\n",
        "    enc_ftrs = self.encoder(x)\n",
        "    out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:]) # 把不同尺度的所有featuremap都輸入decoder，我們在decoder需要做featuremap的拼接\n",
        "    out      = self.head(out)\n",
        "    if self.activation:\n",
        "      out = self.activation(out)\n",
        "    \n",
        "    if self.retain_dim:\n",
        "      out = F.interpolate(out, self.out_sz)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59247206",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59247206",
        "outputId": "81af4fed-5f21-4a71-959c-594b82c0756b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 512, 512])\n"
          ]
        }
      ],
      "source": [
        "unet = UNet(num_class=1, padding = 'same', out_sz=(WIDTH,HEIGHT), retain_dim=False)\n",
        "unet#.to(device)\n",
        "x    = torch.randn(1, 3, WIDTH, HEIGHT)#.to(device)\n",
        "y_pred = unet(x)\n",
        "print(y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac65987",
      "metadata": {
        "id": "4ac65987"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "create_unet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}