{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880c8549",
   "metadata": {},
   "source": [
    "# Unet\n",
    "source: https://amaarora.github.io/2020/09/13/unet.html\n",
    "\n",
    "<img src=\"https://i.imgur.com/LQORH9i.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb07a168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f0ec9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_LABELS = 1\n",
    "WIDTH = 512\n",
    "HEIGHT = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "349bb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b779e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0d765bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # for debug建議使用cpu作為torch的運行背景\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f358e5",
   "metadata": {},
   "source": [
    "## Chapter1 : UNet網路構建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1cd3f",
   "metadata": {},
   "source": [
    "### ConvBlock\n",
    "- 加入Instance Norm.\n",
    "- <img src=\"https://miro.medium.com/max/983/1*p84Hsn4-e60_nZPllkxGZQ.png\" width=\"50%\">\n",
    "\n",
    "> 上圖為一整個batch的feature-map。輸入6張圖片，輸入6chs, 輸出也是6chs(C方向看進去是channel, N方向看進去是圖片)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ef57c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 原始版本\n",
    "# class convBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "#         self.relu  = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7adffbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加入instance normalization\n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, padding = 'same', kernel_size=3):\n",
    "        super().__init__()\n",
    "        kernel_size = kernel_size\n",
    "        pad_size = lambda kernel_size:(kernel_size-1)//2\n",
    "        if padding=='same':\n",
    "            self.padding = pad_size(kernel_size)\n",
    "        else:\n",
    "            self.padding = padding\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, padding=self.padding)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, padding=self.padding)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.INorm(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.INorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f49f728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 512, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = convBlock(1, 64)\n",
    "x = torch.randn(1, 1, WIDTH, HEIGHT)\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834eb58d",
   "metadata": {},
   "source": [
    "## Encoder(DownStream)\n",
    "將影像進行編碼，過程中解析度會縮小(maxpooling、convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac48dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,32,64,128,256,512), padding='same'):\n",
    "        super().__init__()\n",
    "        self.FPN_enc_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding) for i in range(len(chs)-1)])\n",
    "#         self.pool = nn.MaxPool2d(2)\n",
    "        self.pool = torch.max_pool2d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for block in self.FPN_enc_ftrs:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "#             print(x.shape)\n",
    "            x = self.pool(x, kernel_size=2)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f7abcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512, 512])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([1, 32, 512, 512])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "x = torch.randn(1, 3, WIDTH, HEIGHT)\n",
    "features = encoder(x)\n",
    "for f in features:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6c88b",
   "metadata": {},
   "source": [
    "## Decoder(UpStream)\n",
    "將編碼還原成影像，過程中解析度會放大直到回復成輸入影像解析度(transposed Convolution)。\n",
    "- 將編碼還原成影像是因為影像分割是pixel-wise的精度進行預測，解析度被還原後，就可以知道指定pixel位置所對應的類別\n",
    "- 類別資訊通常用feature-map的channels(chs)去劃分，一個channel代表一個class\n",
    "- 有許多UNet模型架構會有輸入576x576，但輸出只有388x388的情況，是因為他們沒有對卷積過程做padding，導致解析度自然下降。最後只要把mask resize到388x388就能繼續計算loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60988889",
   "metadata": {},
   "source": [
    "### Transposed Conv and UpsampleConv\n",
    "<img src=\"https://i.imgur.com/eIIJxre.png\" alt=\"drawing\" width=\"300\"/>\n",
    "<img src=\"https://i.imgur.com/uLo7icF.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Transposed Conv \n",
    "- 透過上面的操作做轉置卷積，feature-map上的數值會作為常數與kernel相乘\n",
    "\n",
    "UpsampleConv\n",
    "- 先做上採樣(Upsample/ Unpooling)\n",
    "- 然後作卷積(padding = same)\n",
    "<!-- #### 替代方案 UpSampling(Unpooling)+Convolution -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2fa2445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 56, 56])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ConvTranspose2d透過設定k=2, s=2, output_padding=0可以讓影像從28x28變成56x56\n",
    "x = torch.randn(1, 3, 28, 28)\n",
    "x = nn.ConvTranspose2d(3, 30, kernel_size=2, stride=2, output_padding=0)(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb71d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class upSampleConvs(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.upSample = nn.Upsample(scale_factor=2)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upSample(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.INorm(x)\n",
    "#         return self.relu(self.conv2(self.relu(self.upSample(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2b6cc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 56, 56])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 28, 28)\n",
    "x = upSampleConvs(3,30)(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc53d1f",
   "metadata": {},
   "source": [
    "### decoder(上採樣) module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5dc0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(512, 256, 128, 64, 32), padding='same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.chs = chs\n",
    "        self.padding = padding\n",
    "#         self.upconvs = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])  # 轉置卷積\n",
    "        self.upconvs = nn.ModuleList([upSampleConvs(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # 上採樣後卷積\n",
    "        self.FPN_dec_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding=padding) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            enc_ftrs = encoder_features[i]\n",
    "            \n",
    "            x = self.upconvs[i](x)\n",
    "                \n",
    "#             if self.padding == 0:\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.FPN_dec_ftrs[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b63fc693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512, 512])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in features:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "184867c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 32, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 512, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "decoder\n",
    "x = torch.randn(1, 512, WIDTH//16, HEIGHT//16)\n",
    "decoder(x, features[::-1][1:]).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72be915",
   "metadata": {},
   "source": [
    "## Unet構建\n",
    "結合encoder和decoder組成Unet。\n",
    "- 在輸出層如果用softmax做多元分類問題預測的話，類別數量要+1(num_classes+background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54011ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_sz, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, padding = 'same'):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs, padding=padding)\n",
    "        self.decoder     = Decoder(dec_chs, padding=padding)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:]) # 把不同尺度的所有featuremap都輸入decoder，我們在decoder需要做featuremap的拼接\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60c7d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512, 512])\n",
      "torch.Size([1, 128, 256, 256])\n",
      "torch.Size([1, 256, 128, 128])\n",
      "torch.Size([1, 512, 64, 64])\n",
      "torch.Size([1, 1024, 32, 32])\n",
      "torch.Size([1, 512, 64, 64])\n",
      "torch.Size([1, 256, 128, 128])\n",
      "torch.Size([1, 128, 256, 256])\n",
      "torch.Size([1, 64, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNet(num_class=1, padding = 'same', out_sz=(WIDTH,HEIGHT), retain_dim=False)\n",
    "unet#.to(device)\n",
    "x    = torch.randn(1, 3, WIDTH, HEIGHT)#.to(device)\n",
    "y_pred = unet(x)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3376f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook unet.ipynb to python\n",
      "[NbConvertApp] Writing 7078 bytes to unet.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if get_ipython().__class__.__name__ =='ZMQInteractiveShell':\n",
    "        os.system('jupyter nbconvert unet.ipynb --to python')\n",
    "\n",
    "# get_ipython().__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9eb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
