{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c809ac",
   "metadata": {},
   "source": [
    "# Unet\n",
    "source: https://amaarora.github.io/2020/09/13/unet.html\n",
    "\n",
    "<img src=\"https://i.imgur.com/LQORH9i.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d85f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_LABELS = 1\n",
    "WIDTH = 512\n",
    "HEIGHT = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f60bd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "732de5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb648dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # for debug建議使用cpu作為torch的運行背景\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b06c56",
   "metadata": {},
   "source": [
    "## Chapter1 : UNet網路構建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876be87",
   "metadata": {},
   "source": [
    "### ConvBlock\n",
    "- 加入Instance Norm.\n",
    "- <img src=\"https://miro.medium.com/max/983/1*p84Hsn4-e60_nZPllkxGZQ.png\" width=\"50%\">\n",
    "\n",
    "> 上圖為一整個batch的feature-map。輸入6張圖片，輸入6chs, 輸出也是6chs(C方向看進去是channel, N方向看進去是圖片)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b4a1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 原始版本\n",
    "# class convBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "#         self.relu  = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acecaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加入instance normalization\n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, padding = 'same', kernel_size=3):\n",
    "        super().__init__()\n",
    "        kernel_size = kernel_size\n",
    "        pad_size = lambda kernel_size:(kernel_size-1)//2\n",
    "        if padding=='same':\n",
    "            self.padding = pad_size(kernel_size)\n",
    "        else:\n",
    "            self.padding = padding\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, padding=self.padding, bias=False)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, padding=self.padding, bias=False)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.INorm(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.INorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "573d4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    block = convBlock(1, 64)\n",
    "    x = torch.randn(1, 1, WIDTH, HEIGHT)\n",
    "    block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f88b80",
   "metadata": {},
   "source": [
    "## Encoder(DownStream)\n",
    "將影像進行編碼，過程中解析度會縮小(maxpooling、convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2588a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,32,64,128,256,512), padding='same'):\n",
    "        super().__init__()\n",
    "        self.FPN_enc_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding) for i in range(len(chs)-1)])\n",
    "#         self.pool = nn.MaxPool2d(2)\n",
    "        self.pool = torch.max_pool2d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for block in self.FPN_enc_ftrs:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "#             print(x.shape)\n",
    "            x = self.pool(x, kernel_size=2)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c95ec0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512, 512])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    encoder = Encoder()\n",
    "    x = torch.randn(1, 3, WIDTH, HEIGHT)\n",
    "    features = encoder(x)\n",
    "    for f in features:\n",
    "        print(f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a435c",
   "metadata": {},
   "source": [
    "## Decoder(UpStream)\n",
    "將編碼還原成影像，過程中解析度會放大直到回復成輸入影像解析度(transposed Convolution)。\n",
    "- 將編碼還原成影像是因為影像分割是pixel-wise的精度進行預測，解析度被還原後，就可以知道指定pixel位置所對應的類別\n",
    "- 類別資訊通常用feature-map的channels(chs)去劃分，一個channel代表一個class\n",
    "- 有許多UNet模型架構會有輸入576x576，但輸出只有388x388的情況，是因為他們沒有對卷積過程做padding，導致解析度自然下降。最後只要把mask resize到388x388就能繼續計算loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bd7912",
   "metadata": {},
   "source": [
    "### Transposed Conv and UpsampleConv\n",
    "<img src=\"https://i.imgur.com/eIIJxre.png\" alt=\"drawing\" width=\"300\"/>\n",
    "<img src=\"https://i.imgur.com/uLo7icF.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Transposed Conv \n",
    "- 透過上面的操作做轉置卷積，feature-map上的數值會作為常數與kernel相乘\n",
    "\n",
    "UpsampleConv\n",
    "- 先做上採樣(Upsample/ Unpooling)\n",
    "- 然後作卷積(padding = same)\n",
    "<!-- #### 替代方案 UpSampling(Unpooling)+Convolution -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7c6cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvTranspose2d透過設定k=2, s=2, output_padding=0可以讓影像從28x28變成56x56\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(1, 3, 28, 28)\n",
    "    x = nn.ConvTranspose2d(3, 30, kernel_size=2, stride=2, output_padding=0)(x)\n",
    "    x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90fef2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleConvs(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.upSample = nn.Upsample(scale_factor=2)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upSample(x)\n",
    "#         x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.INorm(x)\n",
    "#         return self.relu(self.conv2(self.relu(self.upSample(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e8a489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    x = torch.randn(1, 3, 28, 28)\n",
    "    x = UpSampleConvs(3,30)(x)\n",
    "    x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d4b4c",
   "metadata": {},
   "source": [
    "### decoder(上採樣) module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f2b82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(512, 256, 128, 64, 32), padding='same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.chs = chs\n",
    "        self.padding = padding\n",
    "#         self.upconvs = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])  # 轉置卷積\n",
    "        self.upconvs = nn.ModuleList([UpSampleConvs(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # 上採樣後卷積\n",
    "        self.FPN_dec_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding=padding) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            enc_ftrs = encoder_features[i]\n",
    "            \n",
    "            x = self.upconvs[i](x)\n",
    "                \n",
    "#             if self.padding == 0:\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.FPN_dec_ftrs[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb9518ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512, 512])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "torch.Size([1, 128, 128, 128])\n",
      "torch.Size([1, 256, 64, 64])\n",
      "torch.Size([1, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for i in features:\n",
    "        print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f9a96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    decoder = Decoder()\n",
    "    decoder\n",
    "    x = torch.randn(1, 512, WIDTH//16, HEIGHT//16)\n",
    "    decoder(x, features[::-1][1:]).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74c62d",
   "metadata": {},
   "source": [
    "nn.Tanh## Unet構建\n",
    "結合encoder和decoder組成Unet。\n",
    "- 在輸出層如果用softmax做多元分類問題預測的話，類別數量要+1(num_classes+background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95e0cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_sz, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, padding='same', activation=None):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs, padding=padding)\n",
    "        self.decoder     = Decoder(dec_chs, padding=padding)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:]) # 把不同尺度的所有featuremap都輸入decoder，我們在decoder需要做featuremap的拼接\n",
    "        out      = self.head(out)\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        \n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59247206",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    unet = UNet(num_class=1, padding = 'same', out_sz=(WIDTH,HEIGHT), retain_dim=False)\n",
    "    unet#.to(device)\n",
    "    x    = torch.randn(1, 3, WIDTH, HEIGHT)#.to(device)\n",
    "    y_pred = unet(x)\n",
    "    y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ce6ee96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook unet.ipynb to python\n",
      "[NbConvertApp] Writing 7497 bytes to unet.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if get_ipython().__class__.__name__ =='ZMQInteractiveShell':\n",
    "        os.system('jupyter nbconvert unet.ipynb --to python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac65987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
