{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3416ed0d",
   "metadata": {},
   "source": [
    "# Unet\n",
    "source: https://amaarora.github.io/2020/09/13/unet.html\n",
    "\n",
    "<img src=\"https://i.imgur.com/LQORH9i.png\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec493d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_LABELS = 1\n",
    "WIDTH = 128\n",
    "HEIGHT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --user albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594eea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os   \n",
    "cmd = '''mkdir ./data\n",
    "wget -q https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz -O data/MVtech-capsule.tar.xz\n",
    "tar -Jxf data/MVtech-capsule.tar.xz --overwrite --directory ./data\n",
    "'''\n",
    "if not os.path.isdir('./data/capsule'):\n",
    "    for i in cmd.split('\\n'):\n",
    "        os.system(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # for debug建議使用cpu作為torch的運行背景\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4845cd8",
   "metadata": {},
   "source": [
    "## Chapter1 : UNet網路構建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d6854",
   "metadata": {},
   "source": [
    "### ConvBlock\n",
    "- 加入Instance Norm.\n",
    "- <img src=\"https://miro.medium.com/max/983/1*p84Hsn4-e60_nZPllkxGZQ.png\" width=\"50%\">\n",
    "\n",
    "> 上圖為一整個batch的feature-map。輸入6張圖片，輸入6chs, 輸出也是6chs(C方向看進去是channel, N方向看進去是圖片)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49318921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 原始版本\n",
    "# class convBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "#         self.relu  = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加入instance normalization\n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.INorm(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.INorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = convBlock(1, 64)\n",
    "x = torch.randn(1, 1, WIDTH, HEIGHT)\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b0de5",
   "metadata": {},
   "source": [
    "## Encoder(DownStream)\n",
    "將影像進行編碼，過程中解析度會縮小(maxpooling、convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,32,64,128,256,512)):\n",
    "        super().__init__()\n",
    "        self.list_of_blocks = nn.ModuleList([convBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "#         self.pool = nn.MaxPool2d(2)\n",
    "        self.pool = torch.max_pool2d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for block in self.list_of_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "#             print(x.shape)\n",
    "            x = self.pool(x, kernel_size=2)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "x = torch.randn(1, 3, WIDTH, HEIGHT)\n",
    "features = encoder(x)\n",
    "for f in features:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255dcea5",
   "metadata": {},
   "source": [
    "## Decoder(UpStream)\n",
    "將編碼還原成影像，過程中解析度會放大直到回復成輸入影像解析度(transposed Convolution)。\n",
    "- 將編碼還原成影像是因為影像分割是pixel-wise的精度進行預測，解析度被還原後，就可以知道指定pixel位置所對應的類別\n",
    "- 類別資訊通常用feature-map的channels(chs)去劃分，一個channel代表一個class\n",
    "- 有許多UNet模型架構會有輸入576x576，但輸出只有388x388的情況，是因為他們沒有對卷積過程做padding，導致解析度自然下降。最後只要把mask resize到388x388就能繼續計算loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b173b",
   "metadata": {},
   "source": [
    "### Transposed Conv and UpsampleConv\n",
    "<img src=\"https://i.imgur.com/eIIJxre.png\" alt=\"drawing\" width=\"300\"/>\n",
    "<img src=\"https://i.imgur.com/uLo7icF.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "Transposed Conv \n",
    "- 透過上面的操作做轉置卷積，feature-map上的數值會作為常數與kernel相乘\n",
    "\n",
    "UpsampleConv\n",
    "- 先做上採樣(Upsample/ Unpooling)\n",
    "- 然後作卷積(padding = same)\n",
    "<!-- #### 替代方案 UpSampling(Unpooling)+Convolution -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158da6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvTranspose2d透過設定k=2, s=2, output_padding=0可以讓影像從28x28變成56x56\n",
    "x = torch.randn(1, 3, 28, 28)\n",
    "x = nn.ConvTranspose2d(3, 30, kernel_size=2, stride=2, output_padding=0)(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e55166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class upSampleConvs(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.upSample = nn.Upsample(scale_factor=2)\n",
    "        self.conv2 = nn.Conv2d(in_ch, out_ch, 2, padding='same')\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upSample(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.INorm(x)\n",
    "#         return self.relu(self.conv2(self.relu(self.upSample(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2868730a",
   "metadata": {},
   "source": [
    "x = torch.randn(1, 3, 28, 28)\n",
    "x = upSampleConvs(3,30)(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a501a7",
   "metadata": {},
   "source": [
    "### decoder(上採樣) module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(512, 256, 128, 64, 32)):\n",
    "        super().__init__()\n",
    "        self.chs = chs\n",
    "#         self.upconvs = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])  # 轉置卷積\n",
    "        self.upconvs = nn.ModuleList([upSampleConvs(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # 上採樣後卷積\n",
    "        self.list_of_convBlock = nn.ModuleList([convBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x = x\n",
    "            enc_ftrs = encoder_features[i]\n",
    "            enc_ftrs = enc_ftrs\n",
    "            \n",
    "            x = self.upconvs[i](x)\n",
    "\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.list_of_convBlock[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b68ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in features:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()\n",
    "decoder\n",
    "x = torch.randn(1, 512, WIDTH//16, HEIGHT//16)\n",
    "decoder(x, features[::-1][1:]).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f513629",
   "metadata": {},
   "source": [
    "## Unet構建\n",
    "結合encoder和decoder組成Unet。\n",
    "- 在輸出層如果用softmax做多元分類問題預測的話，類別數量要+1(num_classes+background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af52cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(256,256)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:]) # 把不同尺度的所有featuremap都輸入decoder，我們在decoder需要做featuremap的拼接\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(num_class=1)\n",
    "unet.to(device)\n",
    "x    = torch.randn(1, 3, WIDTH, HEIGHT).to(device)\n",
    "unet(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e7945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5298db",
   "metadata": {},
   "source": [
    "## Chapter2 : Test on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da88320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_image_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01faea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 'capsule'\n",
    "path = os.getcwd()\n",
    "img_dir = f'{path}/data/{item}/test/scratch/'\n",
    "print(len(os.listdir(img_dir)))\n",
    "anno_dir = f'{path}/data/{item}/ground_truth/scratch/'\n",
    "\n",
    "defective_number = [i.split('.')[0] for i in os.listdir(img_dir)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728b94c",
   "metadata": {},
   "source": [
    "### 取得image list\n",
    "輸出: data_dic (字典)\n",
    "- key: X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mask_dir = lambda anno_dir, X_lis:[anno_dir+i.split('.')[0]+'_mask.png' for i in X_lis]\n",
    "\n",
    "imgs_path_list = sorted(os.listdir(img_dir))\n",
    "\n",
    "size = 1/len(imgs_path_list)\n",
    "size = 0.1\n",
    "train, test = train_test_split(imgs_path_list, train_size = size)\n",
    "\n",
    "key = 'X_train, X_test, y_train, y_test'.split(', ')\n",
    "\n",
    "lis = []\n",
    "for number in [train, test]:\n",
    "    lis.append([img_dir+i for i in number]) # X\n",
    "for number in [train, test]:\n",
    "    lis.append(mask_dir(anno_dir, number)) # y\n",
    "data_dic = dict(zip(key, lis))\n",
    "data_dic['X_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce76b42",
   "metadata": {},
   "source": [
    "### Build torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, imgs_path_list, anno_path_list, transform=None, target_transform=None):\n",
    "        self.imgs_path_list = imgs_path_list\n",
    "        self.anno_path_list = anno_path_list\n",
    "        if type(imgs_path_list) != list:\n",
    "            raise ValueError('Need Input a list')\n",
    "        if type(anno_path_list) != list:\n",
    "            raise anno_path_list('Need Input a list')\n",
    "        self.transform = transform\n",
    "#   \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path_list)\n",
    "#         return 32\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs_path_list[idx]\n",
    "        file_name = img_path.split('/')[-1].split('.')[0]\n",
    "        mask_path = [i for i in self.anno_path_list if i.__contains__(file_name)][0]\n",
    "        \n",
    "        # cv2\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#       \n",
    "        # cv2\n",
    "        mask = cv2.imread(mask_path,  cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = transform(image=image, mask=mask)\n",
    "            image = transformed['image']\n",
    "            mask = transformed['mask']\n",
    "            \n",
    "        return image, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1be68",
   "metadata": {},
   "source": [
    "#### 補充: 如何從dataset抽image, mask出來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = CustomImageDataset(data_dic['X_train'], data_dic['y_train'])\n",
    "# for idx in range(a.__len__):\n",
    "#     print(idx)\n",
    "\n",
    "#     test_image = a.__getitem__(0)[0]\n",
    "#     test_mask = a.__getitem__(0)[1]\n",
    "#     print(test_image.shape)\n",
    "#     print(test_mask.shape)\n",
    "#     show_image_mask(test_image, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a46a2",
   "metadata": {},
   "source": [
    "#### 補充: 取得指定影像的瑕疵位置(x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import find_objects_contours, center_to_4point\n",
    "# arr = find_objects_contours(test_mask)\n",
    "# print(arr)\n",
    "# points = center_to_4point(test_mask[:, :], arr, WIDTH, pad=50)\n",
    "# print('4 POINTS , WIDTH + pad', points, points[2]-points[0])\n",
    "# x_min, y_min, x_max, y_max = points\n",
    "\n",
    "# show_image_mask(test_mask, test_mask[y_min:y_max, x_min:x_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = data_dic['X_train']\n",
    "file = np.random.choice(file,size=len(file), replace=False)\n",
    "file = [i.split('/')[-1].split('.')[0] for i in file]\n",
    "print(file)\n",
    "img_mask_list = []\n",
    "for i in file[:]:\n",
    "    print(i)\n",
    "    image = cv2.imread(f'/home/jovyan/git/Image_Segmentation/PART2/data/capsule/test/scratch/{i}.png')\n",
    "    print(image.shape)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(f'/home/jovyan/git/Image_Segmentation/PART2/data/capsule/ground_truth/scratch/{i}_mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "#     show_image_mask(image, mask)\n",
    "    \n",
    "    img_mask_list.append((image, mask))\n",
    "len(img_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf26e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
    "from utils import mask_CutMix\n",
    "transform = A.Compose([\n",
    "#     mask_CutMix(img_mask_list, p=0.5),\n",
    "#     A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max, p=0.5),\n",
    "    \n",
    "    A.CenterCrop(300, 900, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=[-0.05, 0.05], p=0.2),\n",
    "    A.Rotate((-30, 30), interpolation=0),  \n",
    "\n",
    "    A.ToFloat(always_apply=True),\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])\n",
    "\n",
    "# target_transform = A.Compose([\n",
    "#     A.ToFloat(always_apply=True),\n",
    "#     A.Resize(WIDTH, HEIGHT),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcdd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = data_dic['X_train']\n",
    "lis = [lis[i%len(lis)] for i in range(BATCH_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建議同時間只有16個(128,128)的sample進行計算 (Total = BATCH_SIZE*MULTIPLE_BATCH)\n",
    "BATCH_SIZE = 16\n",
    "MULTIPLE_BATCH = 1 # [MULTIPLE_BATCH>1]\n",
    "\n",
    "# 在這邊會強制對所有不滿BATCH_SIZE的訓練資料做數量上的匹配，接著透過CustomImageDataset的transform做擴增\n",
    "if len(data_dic['X_train']) < 16: \n",
    "    lis = data_dic['X_train']\n",
    "    lis = [lis[i%len(lis)] for i in range(BATCH_SIZE)]\n",
    "    data_dic['X_train'] = lis\n",
    "\n",
    "dataset_train = CustomImageDataset(data_dic['X_train'], data_dic['y_train'], transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True)\n",
    "# dataloader_train = iter(dataloader_train)\n",
    "\n",
    "dataset_test = CustomImageDataset(data_dic['X_test'], data_dic['y_test'], transform=transform)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724437ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''test transform on dataloader_train'''\n",
    "for data in dataloader_train:\n",
    "#     print(data.shape)\n",
    "#     i = 0\n",
    "    for x, y in zip(*data):\n",
    "#         print(i)\n",
    "#         i += 1\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        show_image_mask(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5745e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "\n",
    "transforms = [ hl.transforms.Prune('Constant') ] # Removes Constant nodes from graph.\n",
    "\n",
    "graph = hl.build_graph(unet, batch.text, transforms=transforms)\n",
    "graph.theme = hl.graph.THEMES['blue'].copy()\n",
    "graph.save('rnn_hiddenlayer', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(num_class=NUM_LABELS)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.Tensor([1000]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "\n",
    "# (optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cd29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('save.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36de3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class patience():\n",
    "    def __init__(self, patience=None):\n",
    "        self.patience = patience\n",
    "        self.record_value = 0\n",
    "        self.early_stop = False\n",
    "        self.ini = True\n",
    "    def record(self):\n",
    "        if self.patience != -1:\n",
    "            if self.ini and not self.patience:\n",
    "                print('No early stop')\n",
    "                self.ini = False\n",
    "                self.patience = -1\n",
    "            else:\n",
    "                if self.record_value >= self.patience:\n",
    "                    print('early stop')\n",
    "                    self.early_stop = True\n",
    "                self.record_value += 1\n",
    "    def reset(self):\n",
    "        self.record_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15ed4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_loss = None\n",
    "# VALUE = None\n",
    "monitor = patience(None)\n",
    "    \n",
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "    torch.cuda.empty_cache()\n",
    "    running_loss = 0.0\n",
    "#     sample_i = 0\n",
    "    \n",
    "    if monitor.early_stop:\n",
    "        break\n",
    "    \n",
    "    multiple_inputs = torch.Tensor([])\n",
    "    multiple_mask = torch.Tensor([])\n",
    "    \n",
    "    for multiple_batch in range(MULTIPLE_BATCH):\n",
    "        for i, data in enumerate(dataloader_train, 1): # get each batch, setting drop_last\n",
    "#             print(i, len(data))\n",
    "            inputs, mask = data\n",
    "#             print(len(inputs))\n",
    "            multiple_inputs = torch.cat((multiple_inputs, inputs),\n",
    "                                        dim=0)\n",
    "            multiple_mask = torch.cat((multiple_mask, mask),\n",
    "                                        dim=0)\n",
    "    inputs = multiple_inputs.to(device)\n",
    "    mask = multiple_mask.to(device)\n",
    "\n",
    "#     print(inputs.shape)\n",
    "    inputs = inputs.permute(0,3,1,2)\n",
    "    mask = mask.unsqueeze(1)\n",
    "    mask = mask.float()/255\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, mask)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    running_loss = running_loss/len(multiple_inputs)\n",
    "    print(f'epochs: {epoch}, average sample loss: {running_loss:.8f}')\n",
    "    \n",
    "    monitor.record()\n",
    "    \n",
    "#     if monitor.record() and running_loss >= best_loss:\n",
    "\n",
    "#     print(best_loss, running_loss)\n",
    "    if not best_loss:\n",
    "        best_loss = running_loss\n",
    "#     elif epoch % 100 == 1:\n",
    "#         torch.save(model, f'{epoch}_save.pt')\n",
    "    elif running_loss < 0.9*best_loss:\n",
    "        best_loss = running_loss\n",
    "        torch.save(model, 'best_save.pt')\n",
    "        monitor.reset()\n",
    "        print('save torch model [best_save.pt]')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "torch.save(model, 'last_save.pt')\n",
    "        \n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312727a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = iter(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = test_set.next()\n",
    "print(image.shape)\n",
    "mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.permute(0,3,1,2).to(device)\n",
    "mask = mask.unsqueeze(1).to(device)\n",
    "mask = mask.float()\n",
    "print(image.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "outputs = model(image)\n",
    "print(outputs.shape)\n",
    "loss = criterion(outputs, mask)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "a = image.permute(0,2,3,1).squeeze(0).cpu().numpy()\n",
    "b = mask.permute(0,2,3,1).squeeze(0).squeeze(-1).cpu().numpy()\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# for x,m in zip(a, b):\n",
    "#     show_image_mask(x, m)\n",
    "    \n",
    "\n",
    "# outputs = model(image)\n",
    "# print(outputs.shape)\n",
    "c = outputs.permute(0,2,3,1).squeeze(0).squeeze(-1).cpu().detach().numpy()\n",
    "# print(c.shape)\n",
    "# b = mask.permute(0,2,3,1).squeeze(0).squeeze(-1).cpu().numpy()\n",
    "# show_image_mask(a, b)\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# show_image_mask(a,b,c)\n",
    "for x, m, p in zip(a, b, c):\n",
    "    show_image_mask(x, m, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5ca4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676aecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa6ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
