{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bd559b",
   "metadata": {
    "id": "21f15bc3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac6e06d",
   "metadata": {
    "id": "7f10bb28"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 導入dicom套件\n",
    "from pydicom import dcmread\n",
    "from pydicom.data import get_testdata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e18a75b",
   "metadata": {
    "id": "H4LL5R2Hqxgy"
   },
   "outputs": [],
   "source": [
    "def show_image(*img_):\n",
    "    for i in img_:\n",
    "        assert i.__class__.__name__ == 'ndarray', 'imput data type should be ndarray'\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for i, img in enumerate(list(img_), 1):\n",
    "        plt.subplot(1,len(img_),i)\n",
    "\n",
    "        if len(np.shape(img)) == 2 or np.shape(img)[-1] == 1:\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        elif len(np.shape(img)) == 3:\n",
    "            plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807405fb",
   "metadata": {
    "id": "d3acdca8"
   },
   "source": [
    "### Build torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df94fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllDataPath(dir_path, imgOnly=False, test_split_size=None):\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(dir_path)):\n",
    "        for file in sorted(files):\n",
    "            if '.dcm' in file:\n",
    "                images.append(os.path.join(root, file))\n",
    "            elif '.png' in file:\n",
    "                labels.append(os.path.join(root, file))\n",
    "    if imgOnly:\n",
    "        data_list = images\n",
    "    else:\n",
    "        data_list = list(zip(images, labels))\n",
    "\n",
    "    if test_split_size:\n",
    "        assert type(test_split_size)==float, 'set float to split test set size'\n",
    "        train, test = train_test_split(data_list,\n",
    "                         test_size = test_split_size)\n",
    "        return {'train':train, 'test':test}\n",
    "    else:\n",
    "        return {'train':data_list}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def6f85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_data train (2299, 2)\n",
      "CT_data test (575, 2)\n",
      "MRI_data train (123, 2)\n",
      "MRI_data test (31, 2)\n",
      "MRI_imgOnly_data train (309,)\n"
     ]
    }
   ],
   "source": [
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/CT/'\n",
    "CT_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_Label/'\n",
    "MRI_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_nonLabel/'\n",
    "MRI_imgOnly_data = getAllDataPath(root, imgOnly=True)\n",
    "\n",
    "for data in ['CT_data', 'MRI_data', 'MRI_imgOnly_data']:\n",
    "    i = eval(data)\n",
    "    for k in i.keys():\n",
    "        print(data,k, np.shape(i[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df0b9da",
   "metadata": {
    "id": "53d773d0"
   },
   "outputs": [],
   "source": [
    "#  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "import cv2\n",
    "class CTMRI_ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_anno_path_list,\n",
    "                 #dtype, \n",
    "#                  dir_path,\n",
    "                 transform=None):\n",
    "        self.imgs_anno_path_list = imgs_anno_path_list\n",
    "        self.transform = transform\n",
    "\n",
    "#   \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_anno_path_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # now = time.time()\n",
    "        imgOnly = False\n",
    "        img_anno_path = self.imgs_anno_path_list[idx]\n",
    "\n",
    "        if type(img_anno_path)==tuple:\n",
    "#             img_anno_path = [i for i in img_anno_path]\n",
    "            image = self.getImg(img_anno_path[0])\n",
    "            mask = self.getImg(img_anno_path[1])\n",
    "        else:\n",
    "            image = self.getImg(img_anno_path)\n",
    "            imgOnly = True\n",
    "    \n",
    "        \n",
    "        if imgOnly:\n",
    "            if self.transform:        \n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "#             image = np.concatenate((image, image, image), axis=0)\n",
    "            image = torch.Tensor(image)\n",
    "            return image\n",
    "        else:\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image, mask=mask)\n",
    "                image = transformed['image']\n",
    "                mask = transformed['mask']\n",
    "#                 print('2', image.max())\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "#             image = np.concatenate((image, image, image), axis=0)\n",
    "            image = torch.Tensor(image)\n",
    "\n",
    "            mask = torch.Tensor(mask) \n",
    "            mask = mask.unsqueeze(0)\n",
    "            return image, mask\n",
    "    \n",
    "    def getImg(self, path):\n",
    "        if path.__contains__('.dcm'):  \n",
    "            # pydcm read image\n",
    "            ds = dcmread(path)\n",
    "            file = ds.pixel_array\n",
    "            # image process\n",
    "            file = cv2.medianBlur(file, 5)\n",
    "            file = cv2.normalize(file, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "        elif path.__contains__('.png'):\n",
    "            file = cv2.imread(path)[...,0]\n",
    "            file = file.astype('float32') # 調整格式以配合albumentation套件需求\n",
    "\n",
    "            if 'MRI' in path:\n",
    "                file[file!=63] = 0\n",
    "                file[file!=0] = 1\n",
    "            elif 'CT' in path:\n",
    "                file /= 255\n",
    "            else:\n",
    "                raise ValueError('Non-support dtype')\n",
    "        else:\n",
    "            raise ValueError(f'img format: {path} unknown')\n",
    "        return file\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdd6443",
   "metadata": {
    "id": "77bc4194"
   },
   "outputs": [],
   "source": [
    "# # https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "WIDTH, HEIGHT = (256,256)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.ToFloat(always_apply=True),\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])\n",
    "\n",
    "target_transform = A.Compose([\n",
    "    A.ToFloat(always_apply=True),\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d43ac",
   "metadata": {
    "id": "Qq7TQNmwzudB"
   },
   "source": [
    "### 建立DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e561ca39",
   "metadata": {
    "id": "d10b3fe6"
   },
   "outputs": [],
   "source": [
    "if '__main__' == __name__:\n",
    "# 建議同時間只有8個(256,256)的sample進行計算 (Total = BATCH_SIZE*MULTIPLE_BATCH)\n",
    "\n",
    "    dataset_train = CTMRI_ImageDataset(MRI_data['train'], transform=transform)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    dataset_test = CTMRI_ImageDataset(MRI_data['test'], transform=target_transform) # **如果要正式使用要記得把這裡換成X_test\n",
    "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    CT_dataset_train = CTMRI_ImageDataset(CT_data['train'], transform=transform)\n",
    "    CT_dataloader_train = torch.utils.data.DataLoader(CT_dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    CT_dataset_test = CTMRI_ImageDataset(CT_data['test'], transform=target_transform)\n",
    "    CT_dataloader_test = torch.utils.data.DataLoader(CT_dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9091c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook dataset.ipynb to python\n",
      "[NbConvertApp] Writing 6386 bytes to dataset.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if '__main__' == __name__:\n",
    "    try:\n",
    "        if get_ipython().__class__.__name__=='ZMQInteractiveShell':\n",
    "            os.system('jupyter nbconvert dataset.ipynb --to python')\n",
    "    except NameError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d409e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CHAOS_CT_MRI_Unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
