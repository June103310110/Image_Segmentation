{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f051ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # 1.9\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c04c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06763d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # for debug建議使用cpu作為torch的運行背景\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ffb5e",
   "metadata": {},
   "source": [
    "## Chapter1 : UNet網路構建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005ea20",
   "metadata": {},
   "source": [
    "### ConvBlock\n",
    "- 加入Instance Norm.\n",
    "- <img src=\"https://miro.medium.com/max/983/1*p84Hsn4-e60_nZPllkxGZQ.png\" 512=\"50%\">\n",
    "\n",
    "> 上圖為一整個batch的feature-map。輸入6張圖片，輸入6chs, 輸出也是6chs(C方向看進去是channel, N方向看進去是圖片)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ee0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 原始版本\n",
    "# class convBlock(nn.Module):\n",
    "#     def __init__(self, in_ch, out_ch):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "#         self.relu  = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4ccc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加入instance normalization\n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, padding = 0):\n",
    "        super().__init__()\n",
    "        kernel_size = kernel_size\n",
    "        pad_same_value = lambda kernel_size:(kernel_size-1)//2\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding, bias=False)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, padding=padding, bias=False)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.INorm(self.conv1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.INorm(self.conv2(x))\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a7c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    block = convBlock(1, 64, 3, padding=1)\n",
    "    WIDTH, HEIGHT = (512, 512)\n",
    "    x = torch.randn(1, 1, WIDTH, HEIGHT)\n",
    "    print(block(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccd27b",
   "metadata": {},
   "source": [
    "## Encoder(DownStream)\n",
    "將影像進行編碼，過程中解析度會縮小(maxpooling、convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6e63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, padding=True):\n",
    "        super().__init__()\n",
    "        pad = 1 if (padding==True or padding=='same') else 0\n",
    "\n",
    "        self.conv = convBlock(in_ch, out_ch, 3, pad)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930b5af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    down = Down(3, 64)\n",
    "    WIDTH, HEIGHT = (512, 512)\n",
    "    \n",
    "    x = torch.randn(1, 3, WIDTH, HEIGHT)\n",
    "    x = down(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430e2c1",
   "metadata": {},
   "source": [
    "## Decoder(UpStream)\n",
    "將編碼還原成影像，過程中解析度會放大直到回復成輸入影像解析度(transposed Convolution)。\n",
    "- 將編碼還原成影像是因為影像分割是pixel-wise的精度進行預測，解析度被還原後，就可以知道指定pixel位置所對應的類別\n",
    "- 類別資訊通常用feature-map的channels(chs)去劃分，一個channel代表一個class\n",
    "- 有許多UNet模型架構會有輸入576x576，但輸出只有388x388的情況，是因為他們沒有對卷積過程做padding，導致解析度自然下降。最後只要把mask resize到388x388就能繼續計算loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073af4a",
   "metadata": {},
   "source": [
    "### Transposed Conv and UpsampleConv\n",
    "<img src=\"https://i.imgur.com/eIIJxre.png\" alt=\"drawing\" 512=\"300\"/>\n",
    "<img src=\"https://i.imgur.com/uLo7icF.png\" alt=\"drawing\" 512=\"300\"/>\n",
    "\n",
    "Transposed Conv \n",
    "- 透過上面的操作做轉置卷積，feature-map上的數值會作為常數與kernel相乘\n",
    "\n",
    "UpsampleConv\n",
    "- 先做上採樣(Upsample/ Unpooling)\n",
    "- 然後作卷積(padding = same)\n",
    "<!-- #### 替代方案 UpSampling(Unpooling)+Convolution -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91948047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True, padding=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            # normal convolutions to reduce the number of channels\n",
    "            self.up = nn.Sequential(nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True),\n",
    "                                    nn.Conv2d(in_ch, (in_ch // 2), 3, padding=1, bias=True))\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, (in_ch // 2), kernel_size = 2, stride = 2)\n",
    "\n",
    "        pad = 1 if (padding==True or padding=='same') else 0\n",
    "        self.conv = convBlock(in_ch, out_ch, 3, padding=pad)\n",
    "    \n",
    "    def forward(self, x, enc_ftrs):\n",
    "        x = self.up(x)\n",
    "        enc_ftrs = self.crop(enc_ftrs, x)\n",
    "        x = torch.cat([x, enc_ftrs], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb55528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    enc_ftrs = torch.randn(1, 256, 32, 32)\n",
    "    x = torch.randn(1, 512, 28, 28)\n",
    "    x = Up(512, 256, bilinear=True, padding=True)(x, enc_ftrs)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7121aab",
   "metadata": {},
   "source": [
    "## Unet構建\n",
    "結合encoder和decoder組成Unet。\n",
    "- 在輸出層如果用softmax做多元分類問題預測的話，類別數量要+1(num_classes+background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9432cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_sz, in_ch, out_ch, bilinear=True, activation=None):\n",
    "        super().__init__()     \n",
    "        if isinstance(out_sz,(int)): self.out_sz = (out_sz, out_sz)\n",
    "        if isinstance(out_sz,(tuple,list)): self.out_sz = tuple(out_sz)\n",
    "                \n",
    "        self.input = convBlock(in_ch, 64, 3, padding=1)\n",
    "        self.head = nn.Conv2d(64, out_ch, 1)\n",
    "        self.activation = activation\n",
    "        self.out_sz = out_sz\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Unet with nn.ModuleList\n",
    "        '''\n",
    "        chs = (64, 128, 256, 512, 1024)\n",
    "        self.down_list = nn.ModuleList([Down(chs[i], chs[i+1], padding='same')for i in range(len(chs)-1)]) \n",
    "        chs = chs[::-1]\n",
    "        self.up_list = nn.ModuleList([Up(chs[i], chs[i+1], padding='same')for i in range(len(chs)-1)]) \n",
    "        \n",
    "        '''\n",
    "        Unet with simple code\n",
    "        ---\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up1 = Up(1024, 512, bilinear)\n",
    "        self.up2 = Up(512, 256, bilinear)\n",
    "        self.up3 = Up(256, 128, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        '''\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        down_layer_0 = self.input(x)\n",
    "        \n",
    "        'Unet with nn.ModuleList'\n",
    "        enc_ftrs = [down_layer_0]\n",
    "        for idx in range(len(self.down_list)):\n",
    "            outputs = self.down_list[idx](enc_ftrs[idx])\n",
    "            enc_ftrs.append(outputs)\n",
    "        enc_ftrs = enc_ftrs[::-1]\n",
    "        \n",
    "        tmp_ftr = enc_ftrs[0]\n",
    "        for idx in range(len(self.up_list)):\n",
    "            tmp_ftr = self.up_list[idx](tmp_ftr, enc_ftrs[idx+1])\n",
    "        \n",
    "        '''\n",
    "        Unet with simple code\n",
    "        ---\n",
    "        down_layer_1 = self.down1(down_layer_0)\n",
    "        down_layer_2 = self.down2(down_layer_1)\n",
    "        down_layer_3 = self.down3(down_layer_2)\n",
    "        down_layer_4 = self.down4(down_layer_3)\n",
    "        up_layer_1 = self.up1(down_layer_4, down_layer_3)\n",
    "        up_layer_2 = self.up2(up_layer_1, down_layer_2)\n",
    "        up_layer_3 = self.up3(up_layer_2, down_layer_1)\n",
    "        tmp_ftr = self.up4(up_layer_3, down_layer_0)\n",
    "        '''\n",
    "\n",
    "\n",
    "        logits = self.head(tmp_ftr)\n",
    "        \n",
    "        # interpolate \n",
    "        _, _, H, W = logits.shape\n",
    "        if (H,W)==self.out_sz: pass\n",
    "        else:\n",
    "            logits = F.interpolate(logits, self.out_sz)\n",
    "        \n",
    "        # add activation (not necessary)\n",
    "        if self.activation:\n",
    "            logits = self.activation(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd82a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    HEIGHT, WIDTH,  = (512, 512)\n",
    "    unet = UNet(HEIGHT, 3, 1, activation=None)\n",
    "    \n",
    "    x    = torch.randn(1, 3, WIDTH, HEIGHT)#.to(device)\n",
    "    y_pred = unet(x)\n",
    "    print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7445055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook unet.ipynb to python\n",
      "[NbConvertApp] Writing 7482 bytes to unet.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if __name__ == '__main__':\n",
    "    if get_ipython().__class__.__name__ =='ZMQInteractiveShell':\n",
    "        os.system('jupyter nbconvert unet.ipynb --to python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4de3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
