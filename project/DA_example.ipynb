{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b71e7b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84bfc5da",
    "outputId": "dd445563-fd75-4276-cf69-e3c0d3edc5f3"
   },
   "outputs": [],
   "source": [
    "# !pip install -q --user albumentations\n",
    "# !pip3 install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3b2bdc",
   "metadata": {
    "id": "21f15bc3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d220fbd",
   "metadata": {
    "id": "7f10bb28"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "\n",
    "\n",
    "# 導入dicom套件\n",
    "from pydicom import dcmread\n",
    "from pydicom.data import get_testdata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72457056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import getAllDataPath, CustomImageDataset, show_image\n",
    "from utils.unet import UNet, ResUnet, AttUnet\n",
    "from utils.loss import DiceLoss, FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae2ba61",
   "metadata": {
    "id": "Egx-Nw5a6qiy"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0506abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])\n",
    "\n",
    "target_transform = A.Compose([                       \n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26a246",
   "metadata": {},
   "source": [
    "## 資料整理與處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c7c5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_data train (2299, 2)\n",
      "CT_data test (575, 2)\n",
      "MRI_data train (49, 2)\n",
      "MRI_data test (13, 2)\n",
      "MRI_nlb_data train (401,)\n"
     ]
    }
   ],
   "source": [
    "root = './data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/CT/'\n",
    "CT_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = './data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_Label/'\n",
    "MRI_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = './data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_nonLabel/'\n",
    "MRI_nlb_data = getAllDataPath(root, test_split_size=None, imgOnly=True)\n",
    "\n",
    "for data in ['CT_data', 'MRI_data', 'MRI_nlb_data']:\n",
    "    i = eval(data)\n",
    "    for k in i.keys():\n",
    "        print(data,k, np.shape(i[k]))\n",
    "\n",
    "\n",
    "dataset_train = CustomImageDataset(MRI_data['train'], transform=transform, pseudo_label=False)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "dataset_test = CustomImageDataset(MRI_data['test'], transform=target_transform, pseudo_label=False) \n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dataset_nlb_train = CustomImageDataset(MRI_nlb_data['train'], transform=transform, pseudo_label=False)\n",
    "dataloader_nlb_train = torch.utils.data.DataLoader(dataset_nlb_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "CT_dataset_train = CustomImageDataset(CT_data['train'], transform=transform, pseudo_label=False)\n",
    "CT_dataloader_train = torch.utils.data.DataLoader(CT_dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "CT_dataset_test = CustomImageDataset(CT_data['test'], transform=target_transform)\n",
    "CT_dataloader_test = torch.utils.data.DataLoader(CT_dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e758c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256]) torch.Size([1, 256, 256])\n",
      "(array([34354,   733,   576,   720, 27788,   627,   611,    60,    43,\n",
      "          24]), array([0.        , 0.09983001, 0.19966002, 0.29949003, 0.39932004,\n",
      "       0.49915004, 0.59898007, 0.69881004, 0.7986401 , 0.89847004,\n",
      "       0.9983001 ], dtype=float32)) tensor([0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Abs: test transform on dataloader_train.\n",
    "---\n",
    "take first image in every batch.\n",
    "'''\n",
    "for data in CT_dataloader_train:\n",
    "    for x, y in zip(*data): \n",
    "        print(x.shape, y.shape)\n",
    "        print(np.histogram(x.numpy()), y.unique())\n",
    "    \n",
    "        show_image(x.squeeze(0).numpy(), y.squeeze(0).numpy())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12493a",
   "metadata": {
    "id": "aAJEsS3ktKP0"
   },
   "source": [
    "## 進行最佳化器與模型設計\n",
    "- 最佳化器(預設使用BECLossWithLogit)\n",
    "- 模型(並將模型放到torch指定的運算空間: [cpu, cuda])\n",
    "\n",
    ">二元分類，只有一個channel的輸出的場合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab247f3",
   "metadata": {
    "id": "uK2_0uYi4N2T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    " \n",
    "    def\tforward(self, input, target):\n",
    "        N = target.size(0)\n",
    "        smooth = 1\n",
    " \n",
    "        input_flat = input.view(N, -1)\n",
    "        target_flat = target.view(N, -1)\n",
    " \n",
    "        intersection = input_flat * target_flat\n",
    " \n",
    "        loss = (2 * intersection.sum(1) + smooth) / (input_flat.sum(1) + target_flat.sum(1) + smooth)\n",
    "        loss = 1 - loss.sum() / N\n",
    " \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d14eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "        This criterion is a implemenation of Focal Loss, which is proposed in \n",
    "        Focal Loss for Dense Object Detection.\n",
    "\n",
    "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "        The losses are averaged across observations for each minibatch.\n",
    "\n",
    "        Args:\n",
    "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), \n",
    "                                   putting more focus on hard, misclassiﬁed examples\n",
    "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "                                However, if the field size_average is set to False, the losses are\n",
    "                                instead summed for each minibatch.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs)\n",
    "\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        #print(class_mask)\n",
    "\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P*class_mask).sum(1).view(-1,1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "        #print('probs size= {}'.format(probs.size()))\n",
    "        #print(probs)\n",
    "\n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
    "        #print('-----bacth_loss------')\n",
    "        #print(batch_loss)\n",
    "\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49037085",
   "metadata": {
    "id": "rOJU1HYFL7eA"
   },
   "outputs": [],
   "source": [
    "class BCEWithRCFLoss(nn.Module):\n",
    "    '''\n",
    "    red: https://github.com/meteorshowers/RCF-pytorch/blob/master/functions.py\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BCEWithRCFLoss, self).__init__()\n",
    "  \n",
    "    def\tforward(self, input, target):\n",
    "        target = target.long()\n",
    "        num_positive = torch.sum((mask==1)).float()\n",
    "        num_negative = torch.sum((mask==0)).float()\n",
    "\n",
    "        mask[mask == 1] = 1.0 * num_negative / (num_positive + num_negative)\n",
    "        mask[mask == 0] = 1.1 * num_positive / (num_positive + num_negative)\n",
    "        cost = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                input.float(),target.float(), weight=mask, reduce=False)\n",
    "        return torch.sum(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb1fd2",
   "metadata": {
    "id": "6d56443d"
   },
   "source": [
    "## 訓練模型 DANN+Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7306ebc5",
   "metadata": {
    "id": "MibQYYfTmLbi"
   },
   "outputs": [],
   "source": [
    "# from unet import convBlock, Encoder, UpSampleConvs, Decoder\n",
    "# from unet import Encoder\n",
    "# from unet import Decoder\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c584552",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加入instance normalization\n",
    "class convBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, padding = 'same', kernel_size=3):\n",
    "        super().__init__()\n",
    "        kernel_size = kernel_size\n",
    "        pad_size = lambda kernel_size:(kernel_size-1)//2\n",
    "        if padding=='same':\n",
    "            self.padding = pad_size(kernel_size)\n",
    "        else:\n",
    "            self.padding = padding\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size, padding=self.padding, bias=False)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size, padding=self.padding, bias=False)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch, affine=True)\n",
    "        self.BN = nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.INorm(self.conv1(x))\n",
    "        x = self.BN(self.conv1(x))\n",
    "        x = self.relu(x)\n",
    "#         x = self.INorm(self.conv2(x))\n",
    "        x = self.BN(self.conv2(x))\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,32,64,128,256,512), padding='same'):\n",
    "        super().__init__()\n",
    "        self.FPN_enc_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding) for i in range(len(chs)-1)])\n",
    "        self.pool = torch.max_pool2d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for block in self.FPN_enc_ftrs:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x, kernel_size=2)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b171b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "class UpSampleConvs(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.upSample = nn.Upsample(scale_factor=2)\n",
    "        self.INorm = torch.nn.InstanceNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upSample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.INorm(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(512, 256, 128, 64, 32), padding='same', multi_level = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.chs = chs\n",
    "        self.padding = padding\n",
    "        self.upconvs = nn.ModuleList([UpSampleConvs(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # 上採樣後卷積\n",
    "        self.FPN_dec_ftrs = nn.ModuleList([convBlock(chs[i], chs[i+1], padding=padding) for i in range(len(chs)-1)]) \n",
    "        self.multi_level = multi_level # int like\n",
    "  \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "#         print(len(encoder_features))\n",
    "        if self.multi_level == 0:\n",
    "            multi_level_Ftrs = []\n",
    "        elif self.multi_level == -1:\n",
    "            multi_level_Ftrs = [0]*(len(encoder_features)+1)\n",
    "            multi_level_Ftrs[0] = x\n",
    "        else:\n",
    "            multi_level_Ftrs = [0]*(self.multi_level+1)\n",
    "            multi_level_Ftrs[0] = x\n",
    "            \n",
    "        for i in range(len(self.chs)-1):\n",
    "            enc_ftrs = encoder_features[i]\n",
    "\n",
    "            x = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.FPN_dec_ftrs[i](x)\n",
    "            if self.multi_level == 0:\n",
    "                pass\n",
    "            elif i<len(multi_level_Ftrs)-1:\n",
    "                multi_level_Ftrs[i+1] = x\n",
    "\n",
    "        return x, multi_level_Ftrs\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2e83f",
   "metadata": {},
   "source": [
    "### 調整unet ch數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05bb1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d6ae8b",
   "metadata": {
    "id": "sXEa2rgVmbS1"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, enc_chs=(1,64,128,256,512,1024), padding='same'):\n",
    "        super().__init__()\n",
    "        tmp = enc_chs[0]\n",
    "        enc_chs = [i//SCALE for i in enc_chs]\n",
    "        enc_chs[0]=tmp\n",
    "        enc_chs = tuple(enc_chs)\n",
    "        self.BN0       = nn.BatchNorm2d(enc_chs[0])\n",
    "        self.encoder     = Encoder(enc_chs, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.BN0(x)\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        return enc_ftrs\n",
    "\n",
    "class LabelPredictor(nn.Module):\n",
    "    def __init__(self, out_sz, num_class=1, retain_dim=False, padding='same',\n",
    "                         activation=None, dec_chs=(1024, 512, 256, 128, 64), multi_level=0):\n",
    "        super().__init__()\n",
    "        dec_chs = tuple(i//SCALE for i in dec_chs)\n",
    "        self.head  = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "        self.activation = activation\n",
    "        self.decoder     = Decoder(dec_chs, multi_level=multi_level, padding=padding)\n",
    "        self.multi_level = multi_level\n",
    "        \n",
    "    def forward(self, enc_ftrs):\n",
    "        \n",
    "        out, multi_level_Ftrs  = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:]) # 把不同尺度的所有featuremap都輸入decoder，我們在decoder需要做featuremap的拼接\n",
    "        out = self.head(out)\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "            \n",
    "        if self.multi_level == 0:\n",
    "            return out, []\n",
    "        else:  \n",
    "            return out, multi_level_Ftrs\n",
    "\n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super().__init__()\n",
    "        self.cell=nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, 3, 1, 1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.cell(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe990452",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "兩個domain的image要送進去，\n",
    "每個domain的shape為: B, in_channels, H, W\n",
    "output的shape是: B, 1, H, W\n",
    "'''\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, level=4, flatten=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        chs = [16*2**i for i in range(level)]\n",
    "        layers = [*discriminator_block(in_channels, 16, normalization=False)]\n",
    "        for i in range(level-1):\n",
    "            layers += discriminator_block(chs[i], chs[i+1])\n",
    "        \n",
    "        if flatten:\n",
    "            self.model = nn.Sequential(*layers,\n",
    "                                       nn.AdaptiveAvgPool2d((1,1)),\n",
    "                                        nn.Flatten(),\n",
    "                                       nn.Linear(16*2**(level-1), 1),\n",
    "#                                        nn.Linear(16*2**(level-1), 128),\n",
    "#                                        nn.Linear(128, 1), \n",
    "#                                        nn.Sigmoid(),\n",
    "                                      )\n",
    "        else:\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, img_A, img_B=False):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "#         img_input = torch.cat((img_A, img_B), 1d\n",
    "        if isinstance(img_B, torch.Tensor):\n",
    "            img_input = torch.cat((img_A, img_B), dim=1)\n",
    "        else:\n",
    "            img_input = img_A\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc04205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, out_sz, out_channels=3, activation=None, multi_level=0):\n",
    "        super().__init__()\n",
    "        self.FeatureExtractor = FeatureExtractor()\n",
    "        LP = LabelPredictor(out_sz=out_sz, activation=activation, multi_level=0)\n",
    "        LP.head = nn.Conv2d(64//SCALE, out_channels, 1)\n",
    "        self.LabelPredictor = LP\n",
    "    def forward(self, x):\n",
    "        x = self.FeatureExtractor(x)\n",
    "        x, _ = self.LabelPredictor(x)\n",
    "        return x, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f88b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, out_sz, out_channels=3, activation=None, multi_level=0):\n",
    "        super().__init__()\n",
    "        self.FeatureExtractor = FeatureExtractor(enc_chs=(1*2,64,128,256))\n",
    "        LP = LabelPredictor(out_sz=out_sz, dec_chs=(256, 128, 64),\n",
    "                            activation=activation, multi_level=multi_level)\n",
    "        LP.head = nn.Conv2d(64//SCALE, out_channels, 1)\n",
    "        self.LabelPredictor = LP\n",
    "        \n",
    "    def forward(self, x, domain_label):\n",
    "        x = torch.cat([x, domain_label], dim=1)\n",
    "\n",
    "        x = self.FeatureExtractor(x)\n",
    "        x, _ = self.LabelPredictor(x)\n",
    "        \n",
    "        return x, _\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b19c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     '''\n",
    "#     netG_CT2MRI = Generator(out_sz=(HEIGHT, WIDTH), out_channels=3,\n",
    "#                              activation=nn.Sigmoid(), multi_level=2,\n",
    "#                             load_path=f'{save_root}share_model_MRI.bin').to(device)\n",
    "\n",
    "#     netG_MRI2CT = Generator(out_sz=(HEIGHT, WIDTH), out_channels=3,\n",
    "#                              activation=nn.Sigmoid(), multi_level=2,\n",
    "#                             load_path=f'{save_root}share_model_MRI.bin').to(device)\n",
    "#     '''\n",
    "#     def __init__(self, out_sz, out_channels=3, activation=None, load_path=None):\n",
    "#         super().__init__()\n",
    "#         self.unet =  Unet(out_sz=(HEIGHT, WIDTH), out_channels=1, activation=None, multi_level=0).to(device)\n",
    "#         if load_path:\n",
    "#             print(f'load model from: {load_path}')\n",
    "#             self.unet.load_state_dict(torch.load(load_path, map_location=device)) \n",
    "#         self.head = nn.Conv2d(1,3,1)\n",
    "#         self.activation = activation\n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.unet(x)\n",
    "#         x = self.head(x)\n",
    "#         x = self.activation(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e4c8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class focal_loss(nn.Module):    \n",
    "#     def __init__(self, alpha=0.25, gamma=2, num_classes = 3, size_average=True):\n",
    "#         \"\"\"\n",
    "#         focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi)      \n",
    "#         步骤详细的实现了 focal_loss损失函数.\n",
    "#         :param alpha:   阿尔法α,类别权重.      当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25\n",
    "#         :param gamma:   伽马γ,难易样本调节参数. retainnet中设置为2\n",
    "#         :param num_classes:     类别数量\n",
    "#         :param size_average:    损失计算方式,默认取均值\n",
    "#         \"\"\"\n",
    "\n",
    "#         super(focal_loss,self).__init__()\n",
    "#         self.size_average = size_average\n",
    "#         if isinstance(alpha,list):\n",
    "#             assert len(alpha)==num_classes   # α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重\n",
    "#             print(\"Focal_loss alpha = {}, 将对每一类权重进行精细化赋值\".format(alpha))\n",
    "#             self.alpha = torch.Tensor(alpha)\n",
    "#         else:\n",
    "#             assert alpha<1   #如果α为一个常数,则降低第一类的影响,在目标检测中为第一类\n",
    "#             print(\" --- Focal_loss alpha = {} ,将对背景类进行衰减,请在目标检测任务中使用 --- \".format(alpha))\n",
    "#             self.alpha = torch.zeros(num_classes)\n",
    "#             self.alpha[0] += alpha\n",
    "#             self.alpha[1:] += (1-alpha) # α 最终为 [ α, 1-α, 1-α, 1-α, 1-α, ...] size:[num_classes]\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, preds, labels):\n",
    "#         \"\"\"\n",
    "#         focal_loss损失计算        \n",
    "#         :param preds:   预测类别. size:[B,N,C] or [B,C]    分别对应与检测与分类任务, B 批次, N检测框数, C类别数        \n",
    "#         :param labels:  实际类别. size:[B,N] or [B]        \n",
    "#         :return:\n",
    "#         \"\"\"        \n",
    "#         # assert preds.dim()==2 and labels.dim()==1        \n",
    "#         preds = preds.view(-1,preds.size(-1))        \n",
    "#         self.alpha = self.alpha.to(preds.device)        \n",
    "#         preds_softmax = F.softmax(preds, dim=1) # 这里并没有直接使用log_softmax, 因为后面会用到softmax的结果(当然你也可以使用log_softmax,然后进行exp操作)        \n",
    "#         preds_logsoft = torch.log(preds_softmax)\n",
    "#         preds_softmax = preds_softmax.gather(1,labels.view(-1,1))   # 这部分实现nll_loss ( crossempty = log_softmax + nll )        \n",
    "#         preds_logsoft = preds_logsoft.gather(1,labels.view(-1,1))        \n",
    "#         self.alpha = self.alpha.gather(0,labels.view(-1))        \n",
    "#         loss = -torch.mul(torch.pow((1-preds_softmax), self.gamma), preds_logsoft)  # torch.pow((1-preds_softmax), self.gamma) 为focal loss中 (1-pt)**γ\n",
    "#         loss = torch.mul(self.alpha, loss.t())        \n",
    "#         if self.size_average:        \n",
    "#             loss = loss.mean()        \n",
    "#         else:            \n",
    "#             loss = loss.sum()        \n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbbc03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PyTorch\n",
    "# ALPHA = 0.8\n",
    "# GAMMA = 2\n",
    "\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, weight=None, size_average=True):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
    "        \n",
    "#         #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "#         inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "#         #flatten label and prediction tensors\n",
    "#         inputs = inputs.view(-1)\n",
    "#         targets = targets.view(-1)\n",
    "        \n",
    "#         #first compute binary cross-entropy \n",
    "#         BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "#         BCE_EXP = torch.exp(-BCE)\n",
    "#         focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "                       \n",
    "#         return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39991821",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/clcarwin/focal_loss_pytorch/blob/e11e75bad957aecf641db6998a1016204722c1bb/focalloss.py\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        if inputs.dim()>2:\n",
    "#             input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "#             input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "#             input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "            C = inputs.shape[1] # num class\n",
    "            inputs = inputs.transpose(1,-1)\n",
    "            inputs = inputs.reshape(-1, C)\n",
    "        \n",
    "        target = target.to(torch.int64)\n",
    "        # flatten all pixel\n",
    "        target = target.view(-1,1) # input should be a tensor (N, 1, H, W), 1 for 1 ch, class shound be [1,C], dtype=Long\n",
    "        \n",
    "        \n",
    "        logpt = F.log_softmax(inputs) # log(softmax(x))\n",
    "        logpt = logpt.gather(1, target) # explain by list, logpt = [logpt[i, target[i]] for i in range(len(target))]\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp()) # reverse the log operation\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=inputs.data.type():\n",
    "                self.alpha = self.alpha.type_as(inputs.data) \n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c053c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "細節手刻: tensor(0.1939)\n",
      "使用Focal Loss class forward: tensor(0.1939)\n"
     ]
    }
   ],
   "source": [
    "# # gather example\n",
    "# 'para'\n",
    "# size_average= True\n",
    "# gamma = 0.1 \n",
    "# alpha = torch.Tensor([0.1, 0.5, 0.3])\n",
    "# num_class = 2\n",
    "\n",
    "# 'input'\n",
    "# sample_x    = torch.zeros(2, num_class, WIDTH, HEIGHT)\n",
    "# x = sample_x.clone()\n",
    "# y =  torch.rand(2, 1, WIDTH, HEIGHT)*num_class\n",
    "# target = y.clone()\n",
    "# target = target.long()\n",
    "\n",
    "# 'operation'\n",
    "# C = x.shape[1]\n",
    "# x = x.transpose(1,-1)\n",
    "# x = x.reshape(-1, C)\n",
    "# logpt = F.log_softmax(x)\n",
    "# # print(logpt)\n",
    "\n",
    "# target = target.view(-1,1)\n",
    "\n",
    "# logpt = logpt.gather(1, target)\n",
    "\n",
    "# ''' # gather opersation explain\n",
    "# will be like this in python:\n",
    "# logpt = [logpt[i][idx] for i, idx in enumerate(target)]\n",
    "\n",
    "# # example\n",
    "# src = list(logpt)\n",
    "# index = list(target)\n",
    "# src = [src[i][idx] for i, idx in enumerate(index)]\n",
    "# print(src)\n",
    "# '''\n",
    "\n",
    "# logpt = logpt.view(-1) # gather後，攤平\n",
    "\n",
    "# pt = Variable(logpt.data.exp())\n",
    "\n",
    "# alpha = torch.Tensor([0.1, 0.5, 0.3]) # 對類別做weight\n",
    "# alpha = alpha.type_as(x.data) \n",
    "# at = alpha.gather(0,target.data.view(-1)) # 對所有樣本產生weight，weight基於target(mask annotation)的index\n",
    "# # print(at) # 已經被攤平了\n",
    "# # print(logpt)\n",
    "# logpt = logpt * Variable(at)\n",
    "# # print(logpt)\n",
    "\n",
    "\n",
    "# loss = -1 * (1-pt)**gamma * logpt\n",
    "# if size_average: \n",
    "#     loss = loss.mean()\n",
    "# else: \n",
    "#     loss = loss.sum()\n",
    "# print('細節手刻:',loss)\n",
    "\n",
    "\n",
    "\n",
    "# x = sample_x.clone()\n",
    "# target = y.long().clone()\n",
    "\n",
    "# a = FocalLoss(gamma=gamma, alpha=alpha)\n",
    "# print('使用Focal Loss class forward:' ,a(x, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a293b79a",
   "metadata": {
    "id": "YHu_BhHfm0X6"
   },
   "outputs": [],
   "source": [
    "# def foo():\n",
    "    \n",
    "#     x    = torch.randn(2, 3, WIDTH, HEIGHT)\n",
    "    \n",
    "# #     a = FeatureExtractor()\n",
    "# #     features = a(x)\n",
    "# #     print('FeatureExtractor\\n', [i.shape for i in features])\n",
    "    \n",
    "# #     a = LabelPredictor(out_sz=(WIDTH, HEIGHT), activation=nn.Sigmoid(), multi_level=0)\n",
    "# #     y_pred, multi_ftrs = a(features)\n",
    "# #     print('LabelPredictor >>>1\\n', y_pred.shape, [i.shape for i in multi_ftrs])\n",
    "    \n",
    "# #     a = LabelPredictor(out_sz=(WIDTH, HEIGHT), activation=nn.Sigmoid(), multi_level=4)\n",
    "# #     y_pred, multi_ftrs = a(features)\n",
    "# #     print('LabelPredictor >>>2\\n', y_pred.shape, [i.shape for i in multi_ftrs])\n",
    "    \n",
    "# #     a  = Discriminator(multi_ftrs[0].shape[1], level=2, flatten=True)\n",
    "# #     logit = a(multi_ftrs[0])\n",
    "# #     print('Discriminator for FeatureExtractor >>>1\\n', multi_ftrs[0].shape[1], logit.shape)\n",
    "    \n",
    "# #     a  = Discriminator(multi_ftrs[1].shape[1], level=4, flatten=True)\n",
    "# #     logit = a(multi_ftrs[1])\n",
    "# #     print('Discriminator for FeatureExtractor >>>2\\n', multi_ftrs[1].shape[1], logit.shape)\n",
    "    \n",
    "# #     a  = Discriminator(multi_ftrs[-1].shape[1], level=4, flatten=True)\n",
    "# #     logit = a(multi_ftrs[-1])\n",
    "# #     print('Discriminator for FeatureExtractor >>>3\\n', multi_ftrs[-1].shape[1], logit.shape)\n",
    "    \n",
    "# #     a = Unet(out_sz=(HEIGHT, WIDTH), out_channels=3, activation=nn.Sigmoid()) \n",
    "# #     pred, _ = a(x)\n",
    "# #     print('Unet\\n', pred.shape)\n",
    "    \n",
    "# #     a  = Discriminator(pred.shape[1], level=2, flatten=True) # input: (2,1,256,256)\n",
    "# #     logit = a(pred)\n",
    "# #      print(logit)\n",
    "# #     print('Discriminator for Unet\\n', logit.shape)\n",
    "    \n",
    "# #    a = Generator(out_sz=(HEIGHT, WIDTH), out_channels=3, activation=nn.Sigmoid(), multi_level=2) \n",
    "# ##    source_domain_tensor = torch.ones(source_data.data.size()).to(device)\n",
    "# #    target_domain_tensor = torch.zeros(source_data.data.size()).to(device)\n",
    "# #    pred, _ = a(x, source_domain_tensor)\n",
    "# #    print(pred.shape)\n",
    "# #    print([i.shape for i in _])\n",
    "# #     print(y_pred.shape)\n",
    "# #     print( F.softmax(y_pred, dim=0).shape)\n",
    "# #     F.softmax(pred_MRI)\n",
    "# #     torch.zeros(source_data.data.size()).to(device)\n",
    "#     class_criterion = FocalLoss(class_num=2)\n",
    "#     loss = class_criterion(x, torch.zeros(x.data.size()))\n",
    "#     print(loss)\n",
    "# foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0abdaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem_params = sum([param.nelement()*param.element_size() for param in a.parameters()])\n",
    "# mem_bufs = sum([buf.nelement()*buf.element_size() for buf in a.buffers()])\n",
    "# mem = mem_params + mem_bufs\n",
    "# mem/1024/1024 #bytes/ kb/ mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af0a90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import sigmoid_focal_loss\n",
    "save_root = './data/save_weights/'\n",
    "# class_criterion = DiceLoss()\n",
    "# class_criterion = FocalLoss()\n",
    "class_criterion = sigmoid_focal_loss\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "consist_criterion = nn.L1Loss()\n",
    "\n",
    "ch_out=1\n",
    "ch_in = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b045a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_D_image_source = Discriminator(in_channels=1, level=3, flatten=True).to(device)\n",
    "model_D_image_target = Discriminator(in_channels=1, level=3, flatten=True).to(device)\n",
    "model_MRI =  Unet(out_sz=(HEIGHT, WIDTH), out_channels=1, activation=None, multi_level=0).to(device)\n",
    "\n",
    "\n",
    "discOptim = optim.Adam([\n",
    "                {'params': model_D_image_source.parameters()},\n",
    "                {'params': model_D_image_target.parameters()},\n",
    "            ], lr=1e-3)  # default lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a880dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disc(backward=True):\n",
    "\n",
    "    for model_D in [model_D_image_source, model_D_image_target]:\n",
    "        for param in model_D.parameters():\n",
    "            param.requires_grad = backward\n",
    "        \n",
    "    \n",
    "    target_domain_label = 0\n",
    "    source_domain_label = 1\n",
    "    \n",
    "    '''\n",
    "    train domain classifier\n",
    "    ---\n",
    "        make domain predction\n",
    "    '''\n",
    "    MRI_pred = model_MRI(target_data)\n",
    "    \n",
    "    target_domain_label = 0\n",
    "    source_domain_label = 1\n",
    "\n",
    "    'prediction discrimator'\n",
    "    domain_logit = model_D_image_source(source_data)\n",
    "    disc_loss = domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device))\n",
    "    \n",
    "    'image discrimator'\n",
    "    domain_logit = model_D_image_target(target_data)\n",
    "    disc_loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device))\n",
    "    \n",
    "    discOptim.zero_grad()\n",
    "    if backward:\n",
    "        disc_loss.backward()\n",
    "        discOptim.step()\n",
    "        discOptim.zero_grad()\n",
    "    return disc_loss/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9ba66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_share_weight_unet(backward=True):# model_MRI/ model_CT\n",
    "\n",
    "    for model in [model_MRI]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = backward\n",
    "    for model_D in [model_D_image_source, model_D_image_target]:\n",
    "        for param in model_D.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "\n",
    "    target_domain_label = 0\n",
    "    '''\n",
    "    compute model_MRI shared weight\n",
    "    '''\n",
    "    mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "    mixed_label = torch.cat([source_label, target_label], dim=0)\n",
    "    \n",
    "    y_pred_mixed, _ = model_MRI(mixed_data) \n",
    "    loss = class_criterion(y_pred_mixed, mixed_label, reduction='mean')\n",
    "    \n",
    "    domain_logit = model_D_image_target(target_data)\n",
    "    disc_loss = domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device))\n",
    "    loss = loss - 0.01*disc_loss\n",
    "    \n",
    "    \n",
    "    if backward:\n",
    "        loss.backward()\n",
    "        tmpOptim.step()\n",
    "        tmpOptim.zero_grad()\n",
    "        \n",
    "        del mixed_data, y_pred_mixed, mixed_label\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3450fff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, testing_loss_value: 0.2874009261528651\n",
      "epoch: 1, testing_loss_value: 0.2805532217025757\n",
      "epoch: 2, testing_loss_value: 0.2758081406354904\n",
      "epoch: 3, testing_loss_value: 0.2728764663139979\n",
      "epoch: 4, testing_loss_value: 0.2679642041524251\n",
      "epoch: 5, testing_loss_value: 0.1868872120976448\n",
      "epoch: 6, testing_loss_value: 0.0880938929816087\n",
      "epoch: 7, testing_loss_value: 0.04701067072649797\n",
      "epoch: 8, testing_loss_value: 0.026667347488303978\n",
      "epoch: 9, testing_loss_value: 0.017499353581418593\n",
      "epoch: 10, testing_loss_value: 0.013018234012027582\n",
      "epoch: 11, testing_loss_value: 0.010967809241265059\n",
      "epoch: 12, testing_loss_value: 0.00978162388006846\n",
      "epoch: 13, testing_loss_value: 0.008856969730307659\n",
      "epoch: 14, testing_loss_value: 0.00772918900474906\n",
      "epoch: 15, testing_loss_value: 0.00681973861840864\n",
      "epoch: 16, testing_loss_value: 0.00728950509801507\n",
      "epoch: 17, testing_loss_value: 0.006872912015145023\n",
      "epoch: 18, testing_loss_value: 0.006078278024991353\n",
      "epoch: 19, testing_loss_value: 0.005751280036444466\n",
      "epoch: 20, testing_loss_value: 0.0061253339517861605\n",
      "epoch: 21, testing_loss_value: 0.006044291580716769\n",
      "epoch: 22, testing_loss_value: 0.005855335155501962\n",
      "epoch: 23, testing_loss_value: 0.005594966312249501\n",
      "epoch: 24, testing_loss_value: 0.0047423016124715405\n",
      "epoch: 25, testing_loss_value: 0.004623413862039645\n",
      "epoch: 26, testing_loss_value: 0.004627175163477659\n",
      "epoch: 27, testing_loss_value: 0.004361310275271535\n",
      "epoch: 28, testing_loss_value: 0.0030342699804653725\n",
      "epoch: 29, testing_loss_value: 0.0037352900641659894\n",
      "epoch: 30, testing_loss_value: 0.004366536624729633\n",
      "epoch: 31, testing_loss_value: 0.004282056742037336\n",
      "epoch: 32, testing_loss_value: 0.0037648198194801807\n",
      "epoch: 33, testing_loss_value: 0.004361471238856514\n",
      "epoch: 34, testing_loss_value: 0.0036427509815742574\n",
      "epoch: 35, testing_loss_value: 0.002462895897527536\n",
      "epoch: 36, testing_loss_value: 0.0031862147928526006\n",
      "epoch: 37, testing_loss_value: 0.0031288464087992907\n",
      "epoch: 38, testing_loss_value: 0.003649679167817036\n",
      "epoch: 39, testing_loss_value: 0.0028493959301461778\n",
      "epoch: 40, testing_loss_value: 0.002532188780605793\n",
      "epoch: 41, testing_loss_value: 0.003197623882442713\n",
      "epoch: 42, testing_loss_value: 0.002452191663905978\n",
      "epoch: 43, testing_loss_value: 0.0032653565673778453\n",
      "epoch: 44, testing_loss_value: 0.00275734377404054\n",
      "epoch: 45, testing_loss_value: 0.002662090972686807\n",
      "epoch: 46, testing_loss_value: 0.0028410300922890506\n",
      "epoch: 47, testing_loss_value: 0.001270178472623229\n",
      "epoch: 48, testing_loss_value: 0.0013438633953531582\n",
      "epoch: 49, testing_loss_value: 0.0025181828532367945\n",
      "epoch: 50, testing_loss_value: 0.0018195738084614277\n",
      "epoch: 51, testing_loss_value: 0.0010419915585468213\n",
      "epoch: 52, testing_loss_value: 0.0035674707032740116\n",
      "epoch: 53, testing_loss_value: 0.002336289267987013\n",
      "epoch: 54, testing_loss_value: 0.0021362704380104938\n",
      "epoch: 55, testing_loss_value: 0.0025021533947438\n",
      "epoch: 56, testing_loss_value: 0.001890134143953522\n",
      "epoch: 57, testing_loss_value: 0.002166746494670709\n",
      "epoch: 58, testing_loss_value: 0.0015587186596045892\n",
      "epoch: 59, testing_loss_value: 0.0022422110196202993\n",
      "epoch: 60, testing_loss_value: 0.0013153224717825651\n",
      "epoch: 61, testing_loss_value: 0.0016405584756284952\n",
      "epoch: 62, testing_loss_value: 0.001939089735969901\n",
      "epoch: 63, testing_loss_value: 0.0012666333156327407\n",
      "epoch: 64, testing_loss_value: 0.0014596196512381236\n",
      "epoch: 65, testing_loss_value: 0.0009263576163599888\n",
      "epoch: 66, testing_loss_value: 0.0013370456484456856\n",
      "epoch: 67, testing_loss_value: 0.00089643569663167\n",
      "epoch: 68, testing_loss_value: 0.0005658126125733057\n",
      "epoch: 69, testing_loss_value: 0.0011639090565343697\n",
      "epoch: 70, testing_loss_value: 0.0004628845490515232\n",
      "epoch: 71, testing_loss_value: 7.178067850569884e-05\n",
      "epoch: 72, testing_loss_value: 0.0004189306637272239\n",
      "epoch: 73, testing_loss_value: -2.4385556268195312e-05\n",
      "epoch: 74, testing_loss_value: 7.219853190084298e-05\n",
      "epoch: 75, testing_loss_value: 8.364064463724692e-05\n",
      "epoch: 76, testing_loss_value: 0.00010019742573300998\n",
      "epoch: 77, testing_loss_value: -0.0001973120185236136\n",
      "epoch: 78, testing_loss_value: -0.0003963494285320242\n",
      "epoch: 79, testing_loss_value: -0.00024215166922658682\n",
      "epoch: 80, testing_loss_value: -0.0008849112006525198\n",
      "epoch: 81, testing_loss_value: -0.0009561484912410378\n",
      "epoch: 82, testing_loss_value: -0.0009495489454517762\n",
      "epoch: 83, testing_loss_value: -0.0006654169022416075\n",
      "epoch: 84, testing_loss_value: -0.0009622526898359259\n",
      "epoch: 85, testing_loss_value: -0.0009251730904603998\n",
      "epoch: 86, testing_loss_value: -0.001267312637840708\n",
      "epoch: 87, testing_loss_value: -0.0012191620965798695\n",
      "epoch: 88, testing_loss_value: -0.0015538283235703905\n",
      "epoch: 89, testing_loss_value: -0.0014123536335925262\n",
      "epoch: 90, testing_loss_value: -0.0011806536155442398\n",
      "epoch: 91, testing_loss_value: -0.0009058441501110792\n",
      "epoch: 92, testing_loss_value: -0.0012545794403801362\n",
      "epoch: 93, testing_loss_value: -0.0009545966361959776\n",
      "epoch: 94, testing_loss_value: -0.0002767635742202401\n",
      "epoch: 95, testing_loss_value: 0.000488964452718695\n",
      "epoch: 96, testing_loss_value: 0.0007542611177389821\n",
      "epoch: 97, testing_loss_value: 0.0004992121830582619\n",
      "epoch: 98, testing_loss_value: 0.0005498407408595085\n",
      "epoch: 99, testing_loss_value: -0.00039712339639663696\n"
     ]
    }
   ],
   "source": [
    "source_dataloader, target_dataloader = CT_dataloader_train, dataloader_train\n",
    "test_dataloader = dataloader_test\n",
    "\n",
    "EPOCHS = 100\n",
    "min_target_loss_value = 100\n",
    "tmpOptim = optim.Adam([{'params': model_MRI.parameters()},\n",
    "                  ], lr=1e-1)\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    share_weight_value = 0.0\n",
    "    testing_loss_value = 0.0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        for param_group in tmpOptim.param_groups:\n",
    "            param_group['lr'] = 1e-5\n",
    "    elif epoch == 5:\n",
    "        for param_group in tmpOptim.param_groups:\n",
    "            param_group['lr'] = 1e-2\n",
    "            \n",
    "    for i, ((source_data, source_label), (target_data, target_label)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "        source_data = source_data.to(device)\n",
    "        source_label = source_label.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "        target_label = target_label.to(device)\n",
    "    \n",
    "        \n",
    "\n",
    "        loss = train_disc(backward=False)\n",
    "        while loss.item()>0.5:\n",
    "            loss = train_disc()\n",
    "\n",
    "  \n",
    "        share_weight_value += train_share_weight_unet().item()\n",
    "        \n",
    "        del source_data, source_label, target_data, target_label\n",
    "        torch.cuda.empty_cache()\n",
    "        print(i, end='\\r')\n",
    "\n",
    "\n",
    "    testing_loss_value = share_weight_value/(i+1)\n",
    "    print(f'epoch: {epoch}, testing_loss_value: {testing_loss_value}')\n",
    "\n",
    "    if testing_loss_value < 0.5:\n",
    "        torch.save(model_MRI.state_dict(), f'{save_root}share_model_MRI.bin')\n",
    "#         break\n",
    "    torch.save(model_MRI.state_dict(), f'{save_root}share_model_MRI.bin')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480816f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea4e592",
   "metadata": {},
   "source": [
    "### GAN disc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cf3e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_MRI =  Unet(out_sz=(HEIGHT, WIDTH), out_channels=ch_out, activation=None, multi_level=0).to(device)\n",
    "model_CT = Unet(out_sz=(HEIGHT, WIDTH), out_channels=ch_out, activation=None, multi_level=0).to(device)\n",
    "\n",
    "model_MRI.load_state_dict(torch.load(f'{save_root}share_model_MRI.bin', map_location=device)) \n",
    "model_CT.load_state_dict(torch.load(f'{save_root}share_model_MRI.bin', map_location=device)) \n",
    "\n",
    "model_D_pred_source = Discriminator(in_channels=ch_out, level=3, flatten=True).to(device)\n",
    "model_D_pred_target = Discriminator(in_channels=ch_out, level=3, flatten=True).to(device)\n",
    "model_D_image_source = Discriminator(in_channels=ch_in, level=3, flatten=True).to(device)\n",
    "model_D_image_target = Discriminator(in_channels=ch_in, level=3, flatten=True).to(device)\n",
    "\n",
    "\n",
    "netG_CT2MRI = Generator(out_sz=(HEIGHT, WIDTH), out_channels=1, activation=None, multi_level=0).to(device) # stacked conv + linear  \n",
    "netG_MRI2CT = Generator(out_sz=(HEIGHT, WIDTH), out_channels=1, activation=None, multi_level=0).to(device)\n",
    "\n",
    "discOptim = optim.Adam([\n",
    "                {'params': model_D_pred_source.parameters()},\n",
    "                {'params': model_D_pred_target.parameters()},\n",
    "                {'params': model_D_image_source.parameters()},\n",
    "                {'params': model_D_image_target.parameters()},\n",
    "            ], lr=1e-4)  # default lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "727cca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disc(backward=True):\n",
    "    for model_D in [model_D_pred_source, model_D_pred_target, model_D_image_source, model_D_image_target]:\n",
    "        for param in model_D.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "#     for model in [netG_CT2MRI, netG_MRI2CT]:\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "    with torch.no_grad():\n",
    "        source_domain_tensor = torch.ones(source_data.data.size()).to(device)\n",
    "        target_domain_tensor = torch.zeros(source_data.data.size()).to(device)\n",
    "        \n",
    "        fake_MRI, _ = netG_CT2MRI(source_data, target_domain_tensor)\n",
    "        fake_CT, _ = netG_MRI2CT(target_data, source_domain_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        CT_pred, _ = model_CT(source_data)\n",
    "        MRI_pred, _ = model_MRI(target_data)\n",
    "        fake_CT_pred, _ = model_CT(fake_CT)\n",
    "        fake_MRI_pred, _ = model_MRI(fake_MRI)\n",
    "    \n",
    "    '''\n",
    "    prediction discrimator\n",
    "    '''    \n",
    "    domain_logit = model_D_pred_source(CT_pred.detach())\n",
    "    loss = domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device))\n",
    "    \n",
    "    domain_logit = model_D_pred_source(fake_CT_pred.detach())\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device))\n",
    "\n",
    "    domain_logit = model_D_pred_target(MRI_pred.detach())\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device))\n",
    "    \n",
    "    domain_logit = model_D_pred_target(fake_MRI_pred.detach())\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device))\n",
    "    \n",
    "    '''\n",
    "    image discrimator\n",
    "    '''\n",
    "    domain_logit = model_D_image_source(source_data)\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device))\n",
    "    \n",
    "    domain_logit = model_D_image_source(fake_CT.detach())\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device))\n",
    "                             \n",
    "    domain_logit = model_D_image_target(target_data)\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device))\n",
    "                             \n",
    "    domain_logit = model_D_image_target(fake_MRI.detach())\n",
    "    loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device))\n",
    "\n",
    "    \n",
    "    if backward:\n",
    "        loss.backward()\n",
    "        discOptim.step()\n",
    "        discOptim.zero_grad()\n",
    "        \n",
    "        del CT_pred, MRI_pred, fake_CT_pred, fake_MRI_pred, fake_MRI, fake_CT, source_domain_tensor, target_domain_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "    loss /= 8\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9fe5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc_loss_value:0.7100507815678915\n",
      "disc_loss_value:0.6850405236085256\n",
      "disc_loss_value:0.6684035460154215\n"
     ]
    }
   ],
   "source": [
    "source_dataloader, target_dataloader = CT_dataloader_train, dataloader_train\n",
    "test_dataloader = dataloader_test\n",
    "\n",
    "EPOCHS = 3\n",
    "min_target_loss_value = 100\n",
    "source_domain_label = 1\n",
    "target_domain_label = 0\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    disc_loss_value = 0.0\n",
    "    for i, ((source_data, source_label), (target_data, target_label)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "        source_data = source_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "    \n",
    "        print(i, end='\\r')\n",
    "        disc_loss_value += train_disc().item()\n",
    "        \n",
    "        del source_data, target_data\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    disc_loss_value /= (i+1)     \n",
    "    print(f'disc_loss_value:{disc_loss_value}')\n",
    "    if disc_loss_value <= 0.6:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a855da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3b5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d821c04",
   "metadata": {},
   "source": [
    "### GAN gen loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c614fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(backward=True):\n",
    "        # part 1\n",
    "    for model in [model_CT, model_MRI]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    for model_D in [model_D_pred_source, model_D_pred_target, model_D_image_source, model_D_image_target]:\n",
    "        for param in model_D.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    for model in [netG_CT2MRI, netG_MRI2CT]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # part2 \n",
    "    source_domain_tensor = torch.ones(source_data.data.size()).to(device)\n",
    "    target_domain_tensor = torch.zeros(target_data.data.size()).to(device)\n",
    "\n",
    "    fake_MRI, _ = netG_CT2MRI(source_data, target_domain_tensor)\n",
    "    fake_CT, _ = netG_MRI2CT(target_data, source_domain_tensor)\n",
    "    \n",
    "    'y pred'\n",
    "    CT_pred, _ = model_CT(source_data)\n",
    "    MRI_pred, _ = model_MRI(target_data)\n",
    "    fake_CT_pred, _ = model_CT(fake_CT)\n",
    "    fake_MRI_pred, _ = model_MRI(fake_MRI)\n",
    "    \n",
    "    # part 3\n",
    "    '''\n",
    "    discrimator loss (domain_loss)\n",
    "    '''\n",
    "    'image'\n",
    "    domain_logit = model_D_image_source(fake_CT) \n",
    "    domain_loss = domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device)) # 給相反的標記\n",
    "    \n",
    "    domain_logit = model_D_image_target(fake_MRI)\n",
    "    domain_loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device)) # 給相反的標記\n",
    "    'pred'\n",
    "    domain_logit = model_D_pred_source(fake_CT_pred)\n",
    "    domain_loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(target_domain_label).to(device)) # 給相反的標記\n",
    "\n",
    "    domain_logit = model_D_pred_target(fake_MRI_pred)\n",
    "    domain_loss += domain_criterion(domain_logit, \n",
    "                           torch.zeros(domain_logit.data.size()).fill_(source_domain_label).to(device)) # 給相反的標記\n",
    "\n",
    "    # part 4\n",
    "    '''\n",
    "    cycle loss (consistency loss)\n",
    "    '''\n",
    "    rec_CT, _ = netG_MRI2CT(fake_MRI, source_domain_tensor)\n",
    "    rec_MRI, _ = netG_CT2MRI(fake_CT, target_domain_tensor)\n",
    "    \n",
    "    csis_loss = consist_criterion(rec_CT, source_data)\n",
    "    csis_loss += consist_criterion(rec_MRI, target_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = domain_loss + csis_loss\n",
    "    if backward:\n",
    "        loss.backward()\n",
    "        unetOptim.step()\n",
    "        unetOptim.zero_grad()\n",
    "        \n",
    "    del CT_pred, MRI_pred, fake_CT_pred, fake_MRI_pred, source_domain_tensor, target_domain_tensor\n",
    "    del fake_MRI, fake_CT, rec_CT, rec_MRI\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return domain_loss/4, csis_loss/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e0ef57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unetOptim = optim.Adam([\n",
    "                {'params': model_MRI.parameters(), 'lr': 1e-1},\n",
    "                {'params': model_CT.parameters()}, # 讓MRI有小一點的lr，ct用default lr\n",
    "                {'params': netG_CT2MRI.parameters()},\n",
    "                {'params': netG_MRI2CT.parameters()},\n",
    "            ], lr=1e-1)  # default lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04b419a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, domain_loss_value:0.6979879438877106, csis_loss_value: 0.30888528376817703\n",
      "epoch: 1, domain_loss_value:0.6663981378078461, csis_loss_value: 0.18158342689275742\n",
      "epoch: 2, domain_loss_value:0.6580269932746887, csis_loss_value: 0.14573676139116287\n",
      "epoch: 3, domain_loss_value:0.6583529512087504, csis_loss_value: 0.11097013826171558\n",
      "epoch: 4, domain_loss_value:0.6546542048454285, csis_loss_value: 0.1172639603416125\n",
      "epoch: 5, domain_loss_value:0.653028279542923, csis_loss_value: 0.11916929607590039\n",
      "epoch: 6, domain_loss_value:0.6481597820917765, csis_loss_value: 0.09114926929275195\n",
      "epoch: 7, domain_loss_value:0.6460451980431875, csis_loss_value: 0.06418837420642376\n",
      "epoch: 8, domain_loss_value:0.6454331576824188, csis_loss_value: 0.056927694007754326\n",
      "epoch: 9, domain_loss_value:0.6447021961212158, csis_loss_value: 0.0474408691128095\n",
      "epoch: 10, domain_loss_value:0.6443780759970347, csis_loss_value: 0.05097734493513902\n",
      "epoch: 11, domain_loss_value:0.6433417598406473, csis_loss_value: 0.05359656487902006\n",
      "epoch: 12, domain_loss_value:0.6431660254796346, csis_loss_value: 0.052068205550312996\n",
      "epoch: 13, domain_loss_value:0.6427085200945536, csis_loss_value: 0.050462389985720314\n",
      "epoch: 14, domain_loss_value:0.642264058192571, csis_loss_value: 0.038598379120230675\n",
      "epoch: 15, domain_loss_value:0.6420452892780304, csis_loss_value: 0.042528433725237846\n",
      "epoch: 16, domain_loss_value:0.6412157913049062, csis_loss_value: 0.04702242153386275\n",
      "epoch: 17, domain_loss_value:0.6406331658363342, csis_loss_value: 0.04182980333765348\n",
      "epoch: 18, domain_loss_value:0.6402482191721598, csis_loss_value: 0.04135508028169473\n",
      "epoch: 19, domain_loss_value:0.6399740080038706, csis_loss_value: 0.03739885954807202\n",
      "epoch: 20, domain_loss_value:0.63984215259552, csis_loss_value: 0.034254780039191246\n",
      "epoch: 21, domain_loss_value:0.640499492486318, csis_loss_value: 0.03323622699826956\n",
      "epoch: 22, domain_loss_value:0.640522301197052, csis_loss_value: 0.03440378544231256\n",
      "epoch: 23, domain_loss_value:0.6404140094916025, csis_loss_value: 0.033959561648468174\n",
      "epoch: 24, domain_loss_value:0.6393860578536987, csis_loss_value: 0.03990214690566063\n",
      "epoch: 25, domain_loss_value:0.6390333076318105, csis_loss_value: 0.03146505821496248\n",
      "epoch: 26, domain_loss_value:0.638504425684611, csis_loss_value: 0.025275394630928833\n",
      "epoch: 27, domain_loss_value:0.6375012497107188, csis_loss_value: 0.0312957384934028\n",
      "epoch: 28, domain_loss_value:0.6372418999671936, csis_loss_value: 0.03702920830498139\n",
      "epoch: 29, domain_loss_value:0.6365155279636383, csis_loss_value: 0.03917664258430401\n",
      "epoch: 30, domain_loss_value:0.6360178589820862, csis_loss_value: 0.03479441503683726\n",
      "epoch: 31, domain_loss_value:0.6356156269709269, csis_loss_value: 0.036225179210305214\n",
      "epoch: 32, domain_loss_value:0.6352187196413676, csis_loss_value: 0.034805006347596645\n",
      "epoch: 33, domain_loss_value:0.6344721515973409, csis_loss_value: 0.027868624155720074\n",
      "epoch: 34, domain_loss_value:0.6337964038054148, csis_loss_value: 0.028194773942232132\n",
      "epoch: 35, domain_loss_value:0.6327752272288004, csis_loss_value: 0.035527560549477734\n",
      "epoch: 36, domain_loss_value:0.6310643553733826, csis_loss_value: 0.03187017639478048\n",
      "epoch: 37, domain_loss_value:0.6300380925337473, csis_loss_value: 0.02806469953308503\n",
      "epoch: 38, domain_loss_value:0.6285666127999624, csis_loss_value: 0.021338264768322308\n",
      "epoch: 39, domain_loss_value:0.6269060373306274, csis_loss_value: 0.03370464127510786\n",
      "epoch: 40, domain_loss_value:0.6256476243336996, csis_loss_value: 0.032944669326146446\n",
      "epoch: 41, domain_loss_value:0.6240884860356649, csis_loss_value: 0.03345526506503423\n",
      "epoch: 42, domain_loss_value:0.6217562556266785, csis_loss_value: 0.03364453030129274\n",
      "epoch: 43, domain_loss_value:0.6201320687929789, csis_loss_value: 0.031944574788212776\n",
      "epoch: 44, domain_loss_value:0.6191726823647817, csis_loss_value: 0.031345633789896965\n",
      "epoch: 45, domain_loss_value:0.6177122394243876, csis_loss_value: 0.02499203197658062\n",
      "epoch: 46, domain_loss_value:0.6165778040885925, csis_loss_value: 0.024609943230946858\n",
      "epoch: 47, domain_loss_value:0.6146207948525747, csis_loss_value: 0.02693589497357607\n",
      "epoch: 48, domain_loss_value:0.6127743721008301, csis_loss_value: 0.03228813596069813\n",
      "epoch: 49, domain_loss_value:0.6116485496362051, csis_loss_value: 0.022047071096797783\n",
      "epoch: 50, domain_loss_value:0.6101989050706228, csis_loss_value: 0.02826817488918702\n",
      "epoch: 51, domain_loss_value:0.6097589830557505, csis_loss_value: 0.03137477052708467\n",
      "epoch: 52, domain_loss_value:0.608689030011495, csis_loss_value: 0.033174616595109306\n",
      "epoch: 53, domain_loss_value:0.6068680187066396, csis_loss_value: 0.032745747516552605\n",
      "epoch: 54, domain_loss_value:0.6069164375464121, csis_loss_value: 0.028171618779500324\n",
      "epoch: 55, domain_loss_value:0.6049375434716543, csis_loss_value: 0.025939413346350193\n",
      "epoch: 56, domain_loss_value:0.6023092865943909, csis_loss_value: 0.030447665291527908\n",
      "epoch: 57, domain_loss_value:0.5995733638604482, csis_loss_value: 0.025220423315962155\n",
      "epoch: 58, domain_loss_value:0.59868124127388, csis_loss_value: 0.028498191696902115\n",
      "epoch: 59, domain_loss_value:0.5968817273775736, csis_loss_value: 0.029336408091088135\n",
      "epoch: 60, domain_loss_value:0.5954354008038839, csis_loss_value: 0.036268141431113086\n",
      "epoch: 61, domain_loss_value:0.5928529997666677, csis_loss_value: 0.0437375046312809\n",
      "epoch: 62, domain_loss_value:0.5906484921773275, csis_loss_value: 0.03836770666142305\n",
      "epoch: 63, domain_loss_value:0.5956754287083944, csis_loss_value: 0.04073857391873995\n",
      "epoch: 64, domain_loss_value:0.5955612361431122, csis_loss_value: 0.03263484997053941\n",
      "epoch: 65, domain_loss_value:0.5936612486839294, csis_loss_value: 0.030068092048168182\n",
      "epoch: 66, domain_loss_value:0.5918939014275869, csis_loss_value: 0.031809333711862564\n",
      "epoch: 67, domain_loss_value:0.5902780195077261, csis_loss_value: 0.037444136726359524\n",
      "epoch: 68, domain_loss_value:0.5854989190896353, csis_loss_value: 0.041786834597587585\n",
      "epoch: 69, domain_loss_value:0.5827935437361399, csis_loss_value: 0.03212965403993925\n",
      "epoch: 70, domain_loss_value:0.5801560084025065, csis_loss_value: 0.038181682427724205\n",
      "epoch: 71, domain_loss_value:0.5786324739456177, csis_loss_value: 0.03262616569797198\n",
      "epoch: 72, domain_loss_value:0.577756255865097, csis_loss_value: 0.028577410926421482\n",
      "epoch: 73, domain_loss_value:0.5763604044914246, csis_loss_value: 0.029258019911746185\n",
      "epoch: 74, domain_loss_value:0.5760294397672018, csis_loss_value: 0.03531236449877421\n",
      "epoch: 75, domain_loss_value:0.5746807356675466, csis_loss_value: 0.04269697330892086\n",
      "epoch: 76, domain_loss_value:0.5741640428702036, csis_loss_value: 0.04559822008013725\n",
      "epoch: 77, domain_loss_value:0.574130137761434, csis_loss_value: 0.03510360699146986\n",
      "epoch: 78, domain_loss_value:0.5730348428090414, csis_loss_value: 0.032291105017066\n",
      "epoch: 79, domain_loss_value:0.5715827743212382, csis_loss_value: 0.03278193653871616\n",
      "epoch: 80, domain_loss_value:0.5710383554299673, csis_loss_value: 0.03991195373237133\n",
      "epoch: 81, domain_loss_value:0.5712147752443949, csis_loss_value: 0.029925264107684296\n",
      "epoch: 82, domain_loss_value:0.5695691307385763, csis_loss_value: 0.032417116376260914\n",
      "epoch: 83, domain_loss_value:0.5689928531646729, csis_loss_value: 0.03025002684444189\n",
      "epoch: 84, domain_loss_value:0.5683646698792776, csis_loss_value: 0.028935193705062073\n",
      "epoch: 85, domain_loss_value:0.5685558219750723, csis_loss_value: 0.03739146515727043\n",
      "epoch: 86, domain_loss_value:0.5679756204287211, csis_loss_value: 0.03906807365516821\n",
      "epoch: 87, domain_loss_value:0.5679642657438914, csis_loss_value: 0.03179706291606029\n",
      "epoch: 88, domain_loss_value:0.5664955675601959, csis_loss_value: 0.032497851798931755\n",
      "epoch: 89, domain_loss_value:0.5659385124842325, csis_loss_value: 0.037788226579626404\n",
      "epoch: 90, domain_loss_value:0.5648172299067179, csis_loss_value: 0.039434446642796196\n",
      "epoch: 91, domain_loss_value:0.5642392635345459, csis_loss_value: 0.03048813280959924\n",
      "epoch: 92, domain_loss_value:0.5650799572467804, csis_loss_value: 0.029111964317659538\n",
      "epoch: 93, domain_loss_value:0.5649789770444235, csis_loss_value: 0.03471031164129575\n",
      "epoch: 94, domain_loss_value:0.5929375390211741, csis_loss_value: 0.11334009716908137\n",
      "epoch: 95, domain_loss_value:0.598765899737676, csis_loss_value: 0.14635335157314935\n",
      "epoch: 96, domain_loss_value:0.5847889880339304, csis_loss_value: 0.16485565652449927\n",
      "epoch: 97, domain_loss_value:0.5827076435089111, csis_loss_value: 0.1105007603764534\n",
      "epoch: 98, domain_loss_value:0.5833467245101929, csis_loss_value: 0.08220793306827545\n",
      "epoch: 99, domain_loss_value:0.5807600816090902, csis_loss_value: 0.0690172662337621\n"
     ]
    }
   ],
   "source": [
    "source_dataloader, target_dataloader = CT_dataloader_train, dataloader_train\n",
    "test_dataloader = dataloader_test\n",
    "\n",
    "EPOCHS = 100\n",
    "min_target_loss_value = 100\n",
    "source_domain_label = 1\n",
    "target_domain_label = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    domain_loss_value = 0.0\n",
    "    csis_loss_value = 0.0\n",
    "    for i, ((source_data, source_label), (target_data, target_label)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "        source_data = source_data.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "\n",
    "        print(i, end='\\r')\n",
    "\n",
    "        a, b = train_gen()\n",
    "        domain_loss_value += a.item()\n",
    "        csis_loss_value += b.item()\n",
    "\n",
    "        del source_data, target_data, a, b\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    domain_loss_value /= (i+1)   \n",
    "    csis_loss_value /= (i+1)   \n",
    "\n",
    "    print(f'epoch: {epoch}, domain_loss_value:{domain_loss_value}, csis_loss_value: {csis_loss_value}')\n",
    "    torch.save(netG_MRI2CT.state_dict(), f'{save_root}netG_MRI2CT.bin')\n",
    "    torch.save(netG_CT2MRI.state_dict(), f'{save_root}netG_CT2MRI.bin')\n",
    "    if domain_loss_value <= 0.4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd271d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e438fc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_CT2MRI.load_state_dict(torch.load(f'{save_root}netG_CT2MRI.bin')) \n",
    "netG_MRI2CT.load_state_dict(torch.load(f'{save_root}netG_MRI2CT.bin')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e4b7e",
   "metadata": {},
   "source": [
    "### nonlabel_unet (model_MRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d2d7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nonlabel_unet(backward=True):\n",
    "    # part1 \n",
    "    for model in [model_CT, model_MRI]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    for model_D in [model_D_pred_source, model_D_pred_target, model_D_image_source, model_D_image_target]:\n",
    "        for param in model_D.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    for model in [netG_CT2MRI, netG_MRI2CT]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    # part 2 \n",
    "    source_domain_tensor = torch.ones(source_data.data.size()).to(device)\n",
    "    target_domain_tensor = torch.zeros(MRI_image.data.size()).to(device)\n",
    "    \n",
    "    fake_MRI, _ = netG_CT2MRI(source_data, target_domain_tensor)\n",
    "    fake_CT, _ = netG_MRI2CT(MRI_image, source_domain_tensor)\n",
    "#     fake_CT_MMRI, _ = netG_MRI2CT(MRI_image, source_domain_tensor)\n",
    "    \n",
    "    'y pred'\n",
    "    CT_pred, _ = model_CT(source_data)\n",
    "    MRI_pred, _ = model_MRI(MRI_image)\n",
    "#     MRI_pred_MMRI, _ = model_MRI(MRI_image)\n",
    "    \n",
    "    fake_CT_pred, _ = model_CT(fake_CT)\n",
    "    fake_MRI_pred, _ = model_MRI(fake_MRI)\n",
    "#     fake_CT_MMRI_pred, _ = model_CT(fake_CT_MMRI)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # part 3 \n",
    "    '''\n",
    "    class loss\n",
    "    '''\n",
    "    class_loss = class_criterion(CT_pred, source_label, reduction='mean')\n",
    "    class_loss += class_criterion(fake_MRI_pred, source_label, reduction='mean')\n",
    "    \n",
    "    # part 4\n",
    "    '''\n",
    "    non-label consistency loss\n",
    "    '''\n",
    "    csis_loss = consist_criterion(MRI_pred, fake_CT_pred)\n",
    "        \n",
    "    # part 5\n",
    "    '''\n",
    "    cycle loss (consistency loss)\n",
    "    '''\n",
    "    rec_CT, _ = netG_MRI2CT(fake_MRI, source_domain_tensor)\n",
    "    rec_MRI, _ = netG_CT2MRI(fake_CT, target_domain_tensor)\n",
    "#     rec_MRI_MMRI, _ = netG_CT2MRI(fake_CT_MMRI, target_domain_tensor)\n",
    "    \n",
    "    cycle_loss = consist_criterion(rec_CT, source_data)\n",
    "    cycle_loss += consist_criterion(rec_MRI, target_data)\n",
    "#     cycle_loss += consist_criterion(rec_MRI_MMRI, MRI_image)\n",
    "    \n",
    "    loss = class_loss + csis_loss + cycle_loss\n",
    "    \n",
    "    \n",
    "    if backward:\n",
    "        loss.backward()\n",
    "        unetOptim.step()\n",
    "        unetOptim.zero_grad()\n",
    "        \n",
    "    del source_domain_tensor, target_domain_tensor\n",
    "    del CT_pred, fake_CT, fake_CT_pred, rec_CT\n",
    "    del MRI_pred, fake_MRI, fake_MRI_pred,  rec_MRI, \n",
    "#     del MRI_pred_MMRI, fake_CT_MMRI_pred, fake_CT_MMRI, rec_MRI_MMRI\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return class_loss/2, csis_loss, cycle_loss/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b62fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_label_unet(backward=True):\n",
    "    # part1 \n",
    "    for model in [model_CT, model_MRI]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # part2 \n",
    "    source_domain_tensor = torch.ones(source_data.data.size()).to(device)\n",
    "    target_domain_tensor = torch.zeros(source_data.data.size()).to(device)\n",
    "    \n",
    "    fake_MRI, _ = netG_CT2MRI(source_data, target_domain_tensor)\n",
    "    fake_CT, _ = netG_MRI2CT(target_data, source_domain_tensor)\n",
    "    \n",
    "    'y pred'\n",
    "    CT_pred, _ = model_CT(source_data)\n",
    "    MRI_pred, _ = model_MRI(target_data)\n",
    "    fake_CT_pred, _ = model_CT(fake_CT)\n",
    "    fake_MRI_pred, _ = model_MRI(fake_MRI)\n",
    "    \n",
    "    # part3 \n",
    "    '''\n",
    "    class loss\n",
    "    '''\n",
    "    class_loss = class_criterion(CT_pred, source_label, reduction='mean')\n",
    "    class_loss += class_criterion(fake_MRI_pred, source_label, reduction='mean')\n",
    "    class_loss += class_criterion(MRI_pred, target_label, reduction='mean')\n",
    "    class_loss += class_criterion(fake_CT_pred, target_label, reduction='mean')\n",
    "    \n",
    "    '''\n",
    "    label consistency loss\n",
    "    '''\n",
    "    csis_loss = consist_criterion(MRI_pred, fake_CT_pred)\n",
    "    csis_loss += consist_criterion(CT_pred, fake_MRI_pred)\n",
    "        \n",
    "    '''\n",
    "    cycle loss (consistency loss)\n",
    "    '''\n",
    "    rec_CT, _ = netG_MRI2CT(fake_MRI, source_domain_tensor)\n",
    "    rec_MRI, _ = netG_CT2MRI(fake_CT, target_domain_tensor)\n",
    "    \n",
    "    cycle_loss = consist_criterion(rec_CT, source_data)\n",
    "    cycle_loss += consist_criterion(rec_MRI, target_data)\n",
    "    \n",
    "    loss = class_loss + csis_loss + cycle_loss\n",
    "    \n",
    "    \n",
    "    if backward:\n",
    "        loss.backward()\n",
    "        unetOptim.step()\n",
    "        unetOptim.zero_grad()\n",
    "        \n",
    "    del CT_pred, MRI_pred, fake_MRI, fake_CT, fake_CT_pred, fake_MRI_pred, source_domain_tensor, target_domain_tensor\n",
    "    del rec_CT, rec_MRI\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return class_loss/4, csis_loss/2, cycle_loss/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4890e62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, class_loss_value:0.0025775176472961903, csis_loss_value: 0.0027542663738131523, cycle_loss_value:0.005803621839731932\n",
      ">>>> testing_loss_value: 0.0025775176472961903\n",
      "epoch: 1, class_loss_value:0.00420988118276, csis_loss_value: 0.006397353485226631, cycle_loss_value:0.009138310328125954\n",
      ">>>> testing_loss_value: 0.00420988118276\n",
      "epoch: 2, class_loss_value:0.004120754078030586, csis_loss_value: 0.008094528689980507, cycle_loss_value:0.005880175158381462\n",
      ">>>> testing_loss_value: 0.004120754078030586\n",
      "epoch: 3, class_loss_value:0.0034504809882491827, csis_loss_value: 0.010210203006863594, cycle_loss_value:0.005766128655523062\n",
      ">>>> testing_loss_value: 0.0034504809882491827\n",
      "epoch: 4, class_loss_value:0.0035048327408730984, csis_loss_value: 0.009605290368199348, cycle_loss_value:0.0054333945736289024\n",
      ">>>> testing_loss_value: 0.0035048327408730984\n",
      "epoch: 5, class_loss_value:0.003669066820293665, csis_loss_value: 0.00945474486798048, cycle_loss_value:0.005949260666966438\n",
      ">>>> testing_loss_value: 0.003669066820293665\n",
      "epoch: 6, class_loss_value:0.0028262296691536903, csis_loss_value: 0.020592067390680313, cycle_loss_value:0.0069425045512616634\n",
      ">>>> testing_loss_value: 0.0028262296691536903\n",
      "epoch: 7, class_loss_value:0.002957169897854328, csis_loss_value: 0.010949898511171341, cycle_loss_value:0.007377238012850285\n",
      ">>>> testing_loss_value: 0.002957169897854328\n",
      "epoch: 8, class_loss_value:0.0038834416773170233, csis_loss_value: 0.00814453698694706, cycle_loss_value:0.0057459305971860886\n",
      ">>>> testing_loss_value: 0.0038834416773170233\n",
      "epoch: 9, class_loss_value:0.0034159370698034763, csis_loss_value: 0.00992392748594284, cycle_loss_value:0.006037108600139618\n",
      ">>>> testing_loss_value: 0.0034159370698034763\n",
      "epoch: 10, class_loss_value:0.0023586915340274572, csis_loss_value: 0.00967327319085598, cycle_loss_value:0.0033890139311552048\n",
      ">>>> testing_loss_value: 0.0023586915340274572\n",
      "epoch: 11, class_loss_value:0.004576703067868948, csis_loss_value: 0.012293565087020397, cycle_loss_value:0.010286224074661732\n",
      ">>>> testing_loss_value: 0.004576703067868948\n",
      "epoch: 12, class_loss_value:0.0037877261638641357, csis_loss_value: 0.012901626527309418, cycle_loss_value:0.006463080644607544\n",
      ">>>> testing_loss_value: 0.0037877261638641357\n",
      "epoch: 13, class_loss_value:0.003993741702288389, csis_loss_value: 0.012601370923221111, cycle_loss_value:0.006406575441360474\n",
      ">>>> testing_loss_value: 0.003993741702288389\n",
      "epoch: 14, class_loss_value:0.003847194369882345, csis_loss_value: 0.017343979328870773, cycle_loss_value:0.0060393474996089935\n",
      ">>>> testing_loss_value: 0.003847194369882345\n",
      "epoch: 15, class_loss_value:0.003838965203613043, csis_loss_value: 0.017475225031375885, cycle_loss_value:0.00583764910697937\n",
      ">>>> testing_loss_value: 0.003838965203613043\n",
      "epoch: 16, class_loss_value:0.0032327035441994667, csis_loss_value: 0.013412452302873135, cycle_loss_value:0.006377560552209616\n",
      ">>>> testing_loss_value: 0.0032327035441994667\n",
      "epoch: 17, class_loss_value:0.00383480335585773, csis_loss_value: 0.011815071105957031, cycle_loss_value:0.006335319485515356\n",
      ">>>> testing_loss_value: 0.00383480335585773\n",
      "epoch: 18, class_loss_value:0.004439376760274172, csis_loss_value: 0.018865574151277542, cycle_loss_value:0.008612243458628654\n",
      ">>>> testing_loss_value: 0.004439376760274172\n",
      "epoch: 19, class_loss_value:0.004221716895699501, csis_loss_value: 0.011440016329288483, cycle_loss_value:0.0066006360575556755\n",
      ">>>> testing_loss_value: 0.004221716895699501\n",
      "epoch: 20, class_loss_value:0.0024146027863025665, csis_loss_value: 0.010277860797941685, cycle_loss_value:0.003912537824362516\n",
      ">>>> testing_loss_value: 0.0024146027863025665\n",
      "epoch: 21, class_loss_value:0.0038615833036601543, csis_loss_value: 0.008911581709980965, cycle_loss_value:0.006141447462141514\n",
      ">>>> testing_loss_value: 0.0038615833036601543\n",
      "epoch: 22, class_loss_value:0.004218871705234051, csis_loss_value: 0.008695744909346104, cycle_loss_value:0.005757675971835852\n",
      ">>>> testing_loss_value: 0.004218871705234051\n",
      "epoch: 23, class_loss_value:0.0036550667136907578, csis_loss_value: 0.008672219701111317, cycle_loss_value:0.005984505172818899\n",
      ">>>> testing_loss_value: 0.0036550667136907578\n",
      "epoch: 24, class_loss_value:0.004313894547522068, csis_loss_value: 0.01429721713066101, cycle_loss_value:0.006402372382581234\n",
      ">>>> testing_loss_value: 0.004313894547522068\n",
      "epoch: 25, class_loss_value:0.0036717939656227827, csis_loss_value: 0.007184172980487347, cycle_loss_value:0.005581618752330542\n",
      ">>>> testing_loss_value: 0.0036717939656227827\n",
      "epoch: 26, class_loss_value:0.003992019221186638, csis_loss_value: 0.017401564866304398, cycle_loss_value:0.005660776048898697\n",
      ">>>> testing_loss_value: 0.003992019221186638\n",
      "epoch: 27, class_loss_value:0.0035396756138652563, csis_loss_value: 0.01580975577235222, cycle_loss_value:0.0055773197673261166\n",
      ">>>> testing_loss_value: 0.0035396756138652563\n",
      "epoch: 28, class_loss_value:0.0041433777660131454, csis_loss_value: 0.019229315221309662, cycle_loss_value:0.005360325798392296\n",
      ">>>> testing_loss_value: 0.0041433777660131454\n",
      "epoch: 29, class_loss_value:0.004204378928989172, csis_loss_value: 0.01493128016591072, cycle_loss_value:0.005993831902742386\n",
      ">>>> testing_loss_value: 0.004204378928989172\n",
      "epoch: 30, class_loss_value:0.002578199841082096, csis_loss_value: 0.011449022218585014, cycle_loss_value:0.0042136050760746\n",
      ">>>> testing_loss_value: 0.002578199841082096\n",
      "epoch: 31, class_loss_value:0.003431718796491623, csis_loss_value: 0.012338805012404919, cycle_loss_value:0.006817693822085857\n",
      ">>>> testing_loss_value: 0.003431718796491623\n",
      "epoch: 32, class_loss_value:0.004538489039987326, csis_loss_value: 0.010968084447085857, cycle_loss_value:0.006015462335199118\n",
      ">>>> testing_loss_value: 0.004538489039987326\n",
      "epoch: 33, class_loss_value:0.003757961094379425, csis_loss_value: 0.013014922849833965, cycle_loss_value:0.00611948873847723\n",
      ">>>> testing_loss_value: 0.003757961094379425\n",
      "epoch: 34, class_loss_value:0.005436311475932598, csis_loss_value: 0.010091373696923256, cycle_loss_value:0.005013167858123779\n",
      ">>>> testing_loss_value: 0.005436311475932598\n",
      "epoch: 35, class_loss_value:0.005684759002178907, csis_loss_value: 0.011555041186511517, cycle_loss_value:0.005843770690262318\n",
      ">>>> testing_loss_value: 0.005684759002178907\n",
      "epoch: 36, class_loss_value:0.00570587208494544, csis_loss_value: 0.008591975085437298, cycle_loss_value:0.005941825918853283\n",
      ">>>> testing_loss_value: 0.00570587208494544\n",
      "epoch: 37, class_loss_value:0.005442204885184765, csis_loss_value: 0.007821140810847282, cycle_loss_value:0.005525414831936359\n",
      ">>>> testing_loss_value: 0.005442204885184765\n",
      "epoch: 38, class_loss_value:0.005888725630939007, csis_loss_value: 0.008423920720815659, cycle_loss_value:0.005914801266044378\n",
      ">>>> testing_loss_value: 0.005888725630939007\n",
      "epoch: 39, class_loss_value:0.0063247582875192165, csis_loss_value: 0.00920826755464077, cycle_loss_value:0.005832045339047909\n",
      ">>>> testing_loss_value: 0.0063247582875192165\n",
      "epoch: 40, class_loss_value:0.002588369883596897, csis_loss_value: 0.006834862288087606, cycle_loss_value:0.0035322844050824642\n",
      ">>>> testing_loss_value: 0.002588369883596897\n",
      "epoch: 41, class_loss_value:0.00786348432302475, csis_loss_value: 0.010476727038621902, cycle_loss_value:0.0054168784990906715\n",
      ">>>> testing_loss_value: 0.00786348432302475\n",
      "epoch: 42, class_loss_value:0.00738453259691596, csis_loss_value: 0.006472793873399496, cycle_loss_value:0.0056421635672450066\n",
      ">>>> testing_loss_value: 0.00738453259691596\n",
      "epoch: 43, class_loss_value:0.005896507762372494, csis_loss_value: 0.00628791656345129, cycle_loss_value:0.006206064485013485\n",
      ">>>> testing_loss_value: 0.005896507762372494\n",
      "epoch: 44, class_loss_value:0.00643148273229599, csis_loss_value: 0.015900760889053345, cycle_loss_value:0.005404296331107616\n",
      ">>>> testing_loss_value: 0.00643148273229599\n",
      "epoch: 45, class_loss_value:0.006780962459743023, csis_loss_value: 0.010282287374138832, cycle_loss_value:0.004984007216989994\n",
      ">>>> testing_loss_value: 0.006780962459743023\n",
      "epoch: 46, class_loss_value:0.006967661902308464, csis_loss_value: 0.008424259722232819, cycle_loss_value:0.005509828682988882\n",
      ">>>> testing_loss_value: 0.006967661902308464\n",
      "epoch: 47, class_loss_value:0.006336270831525326, csis_loss_value: 0.013894423842430115, cycle_loss_value:0.005317942705005407\n",
      ">>>> testing_loss_value: 0.006336270831525326\n",
      "epoch: 48, class_loss_value:0.006737298797816038, csis_loss_value: 0.00931035727262497, cycle_loss_value:0.005640396382659674\n",
      ">>>> testing_loss_value: 0.006737298797816038\n",
      "epoch: 49, class_loss_value:0.005409868434071541, csis_loss_value: 0.00897923856973648, cycle_loss_value:0.005636551883071661\n",
      ">>>> testing_loss_value: 0.005409868434071541\n",
      "epoch: 50, class_loss_value:0.002569558098912239, csis_loss_value: 0.010723660700023174, cycle_loss_value:0.015323682688176632\n",
      ">>>> testing_loss_value: 0.002569558098912239\n",
      "epoch: 51, class_loss_value:0.003859459888190031, csis_loss_value: 0.008447712287306786, cycle_loss_value:0.01743735373020172\n",
      ">>>> testing_loss_value: 0.003859459888190031\n",
      "epoch: 52, class_loss_value:0.004354987293481827, csis_loss_value: 0.012274026870727539, cycle_loss_value:0.018085192888975143\n",
      ">>>> testing_loss_value: 0.004354987293481827\n",
      "epoch: 53, class_loss_value:0.003562960308045149, csis_loss_value: 0.012706547975540161, cycle_loss_value:0.017214495688676834\n",
      ">>>> testing_loss_value: 0.003562960308045149\n",
      "epoch: 54, class_loss_value:0.004249007906764746, csis_loss_value: 0.013465598225593567, cycle_loss_value:0.01746860146522522\n",
      ">>>> testing_loss_value: 0.004249007906764746\n",
      "epoch: 55, class_loss_value:0.005821689032018185, csis_loss_value: 0.014177568256855011, cycle_loss_value:0.017446205019950867\n",
      ">>>> testing_loss_value: 0.005821689032018185\n",
      "epoch: 56, class_loss_value:0.004675291944295168, csis_loss_value: 0.012388026341795921, cycle_loss_value:0.017260471358895302\n",
      ">>>> testing_loss_value: 0.004675291944295168\n",
      "epoch: 57, class_loss_value:0.004640024155378342, csis_loss_value: 0.01674843393266201, cycle_loss_value:0.01674647256731987\n",
      ">>>> testing_loss_value: 0.004640024155378342\n",
      "epoch: 58, class_loss_value:0.00449936930090189, csis_loss_value: 0.01697497069835663, cycle_loss_value:0.016222704201936722\n",
      ">>>> testing_loss_value: 0.00449936930090189\n",
      "epoch: 59, class_loss_value:0.0044686803594231606, csis_loss_value: 0.018371375277638435, cycle_loss_value:0.015970557928085327\n",
      ">>>> testing_loss_value: 0.0044686803594231606\n",
      "epoch: 60, class_loss_value:0.002390435431152582, csis_loss_value: 0.005852345377206802, cycle_loss_value:0.011257616803050041\n",
      ">>>> testing_loss_value: 0.002390435431152582\n",
      "epoch: 61, class_loss_value:0.004840504378080368, csis_loss_value: 0.018298765644431114, cycle_loss_value:0.0120348259806633\n",
      ">>>> testing_loss_value: 0.004840504378080368\n",
      "epoch: 62, class_loss_value:0.005177563987672329, csis_loss_value: 0.011184310540556908, cycle_loss_value:0.009780559688806534\n",
      ">>>> testing_loss_value: 0.005177563987672329\n",
      "epoch: 63, class_loss_value:0.005005951039493084, csis_loss_value: 0.011651861481368542, cycle_loss_value:0.009190872311592102\n",
      ">>>> testing_loss_value: 0.005005951039493084\n",
      "epoch: 64, class_loss_value:0.004725116305053234, csis_loss_value: 0.010579913854598999, cycle_loss_value:0.008658165112137794\n",
      ">>>> testing_loss_value: 0.004725116305053234\n",
      "epoch: 65, class_loss_value:0.004803406074643135, csis_loss_value: 0.008855673484504223, cycle_loss_value:0.008439600467681885\n",
      ">>>> testing_loss_value: 0.004803406074643135\n",
      "epoch: 66, class_loss_value:0.0046137128956615925, csis_loss_value: 0.00750892236828804, cycle_loss_value:0.010706057772040367\n",
      ">>>> testing_loss_value: 0.0046137128956615925\n",
      "epoch: 67, class_loss_value:0.004676089622080326, csis_loss_value: 0.009402467869222164, cycle_loss_value:0.010362960398197174\n",
      ">>>> testing_loss_value: 0.004676089622080326\n",
      "epoch: 68, class_loss_value:0.004546036943793297, csis_loss_value: 0.012172006070613861, cycle_loss_value:0.009861216880381107\n",
      ">>>> testing_loss_value: 0.004546036943793297\n",
      "epoch: 69, class_loss_value:0.004604270216077566, csis_loss_value: 0.012435737065970898, cycle_loss_value:0.008504475466907024\n",
      ">>>> testing_loss_value: 0.004604270216077566\n",
      "epoch: 70, class_loss_value:0.002229397650808096, csis_loss_value: 0.00521658081561327, cycle_loss_value:0.005424174480140209\n",
      ">>>> testing_loss_value: 0.002229397650808096\n",
      "epoch: 71, class_loss_value:0.005321618169546127, csis_loss_value: 0.02194204553961754, cycle_loss_value:0.0073315054178237915\n",
      ">>>> testing_loss_value: 0.005321618169546127\n",
      "epoch: 72, class_loss_value:0.004901038017123938, csis_loss_value: 0.009176637977361679, cycle_loss_value:0.009044231846928596\n",
      ">>>> testing_loss_value: 0.004901038017123938\n",
      "epoch: 73, class_loss_value:0.005332939326763153, csis_loss_value: 0.009332709945738316, cycle_loss_value:0.007640048395842314\n",
      ">>>> testing_loss_value: 0.005332939326763153\n",
      "epoch: 74, class_loss_value:0.0047675929963588715, csis_loss_value: 0.010573484003543854, cycle_loss_value:0.006745394319295883\n",
      ">>>> testing_loss_value: 0.0047675929963588715\n",
      "epoch: 75, class_loss_value:0.004477288573980331, csis_loss_value: 0.009305567480623722, cycle_loss_value:0.006443386431783438\n",
      ">>>> testing_loss_value: 0.004477288573980331\n",
      "epoch: 76, class_loss_value:0.004540713503956795, csis_loss_value: 0.008123225532472134, cycle_loss_value:0.0077350204810500145\n",
      ">>>> testing_loss_value: 0.004540713503956795\n",
      "epoch: 77, class_loss_value:0.005004545673727989, csis_loss_value: 0.00677277147769928, cycle_loss_value:0.00807148776948452\n",
      ">>>> testing_loss_value: 0.005004545673727989\n",
      "epoch: 78, class_loss_value:0.004889405332505703, csis_loss_value: 0.008615597151219845, cycle_loss_value:0.0069219502620399\n",
      ">>>> testing_loss_value: 0.004889405332505703\n",
      "epoch: 79, class_loss_value:0.00411207415163517, csis_loss_value: 0.012622754089534283, cycle_loss_value:0.007289705332368612\n",
      ">>>> testing_loss_value: 0.00411207415163517\n",
      "epoch: 80, class_loss_value:0.002668481320142746, csis_loss_value: 0.004554800223559141, cycle_loss_value:0.018764756619930267\n",
      ">>>> testing_loss_value: 0.002668481320142746\n",
      "epoch: 81, class_loss_value:0.0039804126136004925, csis_loss_value: 0.014268888160586357, cycle_loss_value:0.01969972997903824\n",
      ">>>> testing_loss_value: 0.0039804126136004925\n",
      "epoch: 82, class_loss_value:0.004008845891803503, csis_loss_value: 0.019278787076473236, cycle_loss_value:0.023755094036459923\n",
      ">>>> testing_loss_value: 0.004008845891803503\n",
      "epoch: 83, class_loss_value:0.004197532311081886, csis_loss_value: 0.0268387533724308, cycle_loss_value:0.021286245435476303\n",
      ">>>> testing_loss_value: 0.004197532311081886\n",
      "epoch: 84, class_loss_value:0.0048252297565341, csis_loss_value: 0.009411383420228958, cycle_loss_value:0.019125718623399734\n",
      ">>>> testing_loss_value: 0.0048252297565341\n",
      "epoch: 85, class_loss_value:0.0041049933061003685, csis_loss_value: 0.01904124580323696, cycle_loss_value:0.01877031847834587\n",
      ">>>> testing_loss_value: 0.0041049933061003685\n",
      "epoch: 86, class_loss_value:0.004269780591130257, csis_loss_value: 0.009550809860229492, cycle_loss_value:0.018270831555128098\n",
      ">>>> testing_loss_value: 0.004269780591130257\n",
      "epoch: 87, class_loss_value:0.003951616585254669, csis_loss_value: 0.01162196695804596, cycle_loss_value:0.017845313996076584\n",
      ">>>> testing_loss_value: 0.003951616585254669\n",
      "epoch: 88, class_loss_value:0.004373476840555668, csis_loss_value: 0.012507840991020203, cycle_loss_value:0.01714184507727623\n",
      ">>>> testing_loss_value: 0.004373476840555668\n",
      "epoch: 89, class_loss_value:0.0044004786759614944, csis_loss_value: 0.010990278795361519, cycle_loss_value:0.017773784697055817\n",
      ">>>> testing_loss_value: 0.0044004786759614944\n",
      "epoch: 90, class_loss_value:0.0028438284061849117, csis_loss_value: 0.008752514608204365, cycle_loss_value:0.015178894624114037\n",
      ">>>> testing_loss_value: 0.0028438284061849117\n",
      "epoch: 91, class_loss_value:0.004432495217770338, csis_loss_value: 0.024574628099799156, cycle_loss_value:0.018458208069205284\n",
      ">>>> testing_loss_value: 0.004432495217770338\n",
      "epoch: 92, class_loss_value:0.0039033088833093643, csis_loss_value: 0.018421445041894913, cycle_loss_value:0.017619535326957703\n",
      ">>>> testing_loss_value: 0.0039033088833093643\n",
      "epoch: 93, class_loss_value:0.004697054158896208, csis_loss_value: 0.026435552164912224, cycle_loss_value:0.01791762374341488\n",
      ">>>> testing_loss_value: 0.004697054158896208\n",
      "epoch: 94, class_loss_value:0.003844594117254019, csis_loss_value: 0.027601998299360275, cycle_loss_value:0.017690155655145645\n",
      ">>>> testing_loss_value: 0.003844594117254019\n",
      "epoch: 95, class_loss_value:0.0042714765295386314, csis_loss_value: 0.023105336353182793, cycle_loss_value:0.01756071299314499\n",
      ">>>> testing_loss_value: 0.0042714765295386314\n",
      "epoch: 96, class_loss_value:0.004317059181630611, csis_loss_value: 0.011310561560094357, cycle_loss_value:0.017662830650806427\n",
      ">>>> testing_loss_value: 0.004317059181630611\n",
      "epoch: 97, class_loss_value:0.004161927383393049, csis_loss_value: 0.00937013141810894, cycle_loss_value:0.01683715730905533\n",
      ">>>> testing_loss_value: 0.004161927383393049\n",
      "epoch: 98, class_loss_value:0.004102179314941168, csis_loss_value: 0.017401263117790222, cycle_loss_value:0.017786160111427307\n",
      ">>>> testing_loss_value: 0.004102179314941168\n",
      "epoch: 99, class_loss_value:0.004559582099318504, csis_loss_value: 0.01685061678290367, cycle_loss_value:0.017905402928590775\n",
      ">>>> testing_loss_value: 0.004559582099318504\n",
      "epoch: 100, class_loss_value:0.002922627842053771, csis_loss_value: 0.011952425353229046, cycle_loss_value:0.014335663989186287\n",
      ">>>> testing_loss_value: 0.002922627842053771\n",
      "epoch: 101, class_loss_value:0.009700339287519455, csis_loss_value: 0.01762484386563301, cycle_loss_value:0.01772642694413662\n",
      ">>>> testing_loss_value: 0.009700339287519455\n",
      "epoch: 102, class_loss_value:0.008204147219657898, csis_loss_value: 0.019576353952288628, cycle_loss_value:0.01772192493081093\n",
      ">>>> testing_loss_value: 0.008204147219657898\n",
      "epoch: 103, class_loss_value:0.009434711188077927, csis_loss_value: 0.014014961197972298, cycle_loss_value:0.01771167293190956\n",
      ">>>> testing_loss_value: 0.009434711188077927\n",
      "epoch: 104, class_loss_value:0.008690114133059978, csis_loss_value: 0.01941755786538124, cycle_loss_value:0.01789325661957264\n",
      ">>>> testing_loss_value: 0.008690114133059978\n",
      "epoch: 105, class_loss_value:0.009217905811965466, csis_loss_value: 0.011502690613269806, cycle_loss_value:0.018899518996477127\n",
      ">>>> testing_loss_value: 0.009217905811965466\n",
      "epoch: 106, class_loss_value:0.009117985144257545, csis_loss_value: 0.008676314726471901, cycle_loss_value:0.017759673297405243\n",
      ">>>> testing_loss_value: 0.009117985144257545\n",
      "epoch: 107, class_loss_value:0.009229794144630432, csis_loss_value: 0.008902446366846561, cycle_loss_value:0.016562148928642273\n",
      ">>>> testing_loss_value: 0.009229794144630432\n",
      "epoch: 108, class_loss_value:0.009362244978547096, csis_loss_value: 0.009894328191876411, cycle_loss_value:0.01631775312125683\n",
      ">>>> testing_loss_value: 0.009362244978547096\n",
      "epoch: 109, class_loss_value:0.00919383391737938, csis_loss_value: 0.01333426684141159, cycle_loss_value:0.01787109300494194\n",
      ">>>> testing_loss_value: 0.00919383391737938\n",
      "epoch: 110, class_loss_value:0.003742718603461981, csis_loss_value: 0.008569160476326942, cycle_loss_value:0.015108397230505943\n",
      ">>>> testing_loss_value: 0.003742718603461981\n",
      "epoch: 111, class_loss_value:0.005069914739578962, csis_loss_value: 0.017722569406032562, cycle_loss_value:0.0174654982984066\n",
      ">>>> testing_loss_value: 0.005069914739578962\n",
      "epoch: 112, class_loss_value:0.008111166767776012, csis_loss_value: 0.007188774645328522, cycle_loss_value:0.017771655693650246\n",
      ">>>> testing_loss_value: 0.008111166767776012\n",
      "epoch: 113, class_loss_value:0.011792995035648346, csis_loss_value: 0.014213019981980324, cycle_loss_value:0.018187064677476883\n",
      ">>>> testing_loss_value: 0.011792995035648346\n",
      "epoch: 114, class_loss_value:0.00856650248169899, csis_loss_value: 0.01569884642958641, cycle_loss_value:0.01725519448518753\n",
      ">>>> testing_loss_value: 0.00856650248169899\n",
      "epoch: 115, class_loss_value:0.008902646601200104, csis_loss_value: 0.012151474133133888, cycle_loss_value:0.018297038972377777\n",
      ">>>> testing_loss_value: 0.008902646601200104\n",
      "epoch: 116, class_loss_value:0.009450132958590984, csis_loss_value: 0.0122152641415596, cycle_loss_value:0.017632950097322464\n",
      ">>>> testing_loss_value: 0.009450132958590984\n",
      "epoch: 117, class_loss_value:0.00852257665246725, csis_loss_value: 0.01178016047924757, cycle_loss_value:0.01710238680243492\n",
      ">>>> testing_loss_value: 0.00852257665246725\n",
      "epoch: 118, class_loss_value:0.009420650079846382, csis_loss_value: 0.011820435523986816, cycle_loss_value:0.0176413394510746\n",
      ">>>> testing_loss_value: 0.009420650079846382\n",
      "epoch: 119, class_loss_value:0.008795942179858685, csis_loss_value: 0.00883973017334938, cycle_loss_value:0.01822771318256855\n",
      ">>>> testing_loss_value: 0.008795942179858685\n",
      "epoch: 120, class_loss_value:0.0036348807625472546, csis_loss_value: 0.0081859789788723, cycle_loss_value:0.015524955466389656\n",
      ">>>> testing_loss_value: 0.0036348807625472546\n",
      "epoch: 121, class_loss_value:0.005992551799863577, csis_loss_value: 0.015960749238729477, cycle_loss_value:0.018401088193058968\n",
      ">>>> testing_loss_value: 0.005992551799863577\n",
      "epoch: 122, class_loss_value:0.0068108392879366875, csis_loss_value: 0.005778248887509108, cycle_loss_value:0.01745140738785267\n",
      ">>>> testing_loss_value: 0.0068108392879366875\n",
      "epoch: 123, class_loss_value:0.010923962108790874, csis_loss_value: 0.019407568499445915, cycle_loss_value:0.017994746565818787\n",
      ">>>> testing_loss_value: 0.010923962108790874\n",
      "epoch: 124, class_loss_value:0.009439371526241302, csis_loss_value: 0.015149733051657677, cycle_loss_value:0.017231900244951248\n",
      ">>>> testing_loss_value: 0.009439371526241302\n",
      "epoch: 125, class_loss_value:0.008371105417609215, csis_loss_value: 0.02009989507496357, cycle_loss_value:0.01781240850687027\n",
      ">>>> testing_loss_value: 0.008371105417609215\n",
      "epoch: 126, class_loss_value:0.009390147402882576, csis_loss_value: 0.013675039634108543, cycle_loss_value:0.017826247960329056\n",
      ">>>> testing_loss_value: 0.009390147402882576\n",
      "epoch: 127, class_loss_value:0.009432746097445488, csis_loss_value: 0.022444603964686394, cycle_loss_value:0.01739439368247986\n",
      ">>>> testing_loss_value: 0.009432746097445488\n",
      "epoch: 128, class_loss_value:0.009251715615391731, csis_loss_value: 0.009188852272927761, cycle_loss_value:0.017808614298701286\n",
      ">>>> testing_loss_value: 0.009251715615391731\n",
      "epoch: 129, class_loss_value:0.009477995336055756, csis_loss_value: 0.013522419147193432, cycle_loss_value:0.01725194975733757\n",
      ">>>> testing_loss_value: 0.009477995336055756\n",
      "epoch: 130, class_loss_value:0.004049084149301052, csis_loss_value: 0.006762709002941847, cycle_loss_value:0.01508386991918087\n",
      ">>>> testing_loss_value: 0.004049084149301052\n",
      "epoch: 131, class_loss_value:0.006807532161474228, csis_loss_value: 0.014821367338299751, cycle_loss_value:0.017412228509783745\n",
      ">>>> testing_loss_value: 0.006807532161474228\n",
      "epoch: 132, class_loss_value:0.010451871901750565, csis_loss_value: 0.013256937265396118, cycle_loss_value:0.017330996692180634\n",
      ">>>> testing_loss_value: 0.010451871901750565\n",
      "epoch: 133, class_loss_value:0.011011718772351742, csis_loss_value: 0.012425435706973076, cycle_loss_value:0.017178833484649658\n",
      ">>>> testing_loss_value: 0.011011718772351742\n",
      "epoch: 134, class_loss_value:0.008028661832213402, csis_loss_value: 0.009093026630580425, cycle_loss_value:0.016975505277514458\n",
      ">>>> testing_loss_value: 0.008028661832213402\n",
      "epoch: 135, class_loss_value:0.0083124665543437, csis_loss_value: 0.012654511258006096, cycle_loss_value:0.017512906342744827\n",
      ">>>> testing_loss_value: 0.0083124665543437\n",
      "epoch: 136, class_loss_value:0.008542929776012897, csis_loss_value: 0.014696678146719933, cycle_loss_value:0.01798725686967373\n",
      ">>>> testing_loss_value: 0.008542929776012897\n",
      "epoch: 137, class_loss_value:0.008729798719286919, csis_loss_value: 0.015211954712867737, cycle_loss_value:0.017779119312763214\n",
      ">>>> testing_loss_value: 0.008729798719286919\n",
      "epoch: 138, class_loss_value:0.00880267471075058, csis_loss_value: 0.01602240651845932, cycle_loss_value:0.018910076469182968\n",
      ">>>> testing_loss_value: 0.00880267471075058\n",
      "epoch: 139, class_loss_value:0.008486619219183922, csis_loss_value: 0.00720268115401268, cycle_loss_value:0.017330175265669823\n",
      ">>>> testing_loss_value: 0.008486619219183922\n",
      "epoch: 140, class_loss_value:0.004391661379486322, csis_loss_value: 0.006993533577769995, cycle_loss_value:0.014490321278572083\n",
      ">>>> testing_loss_value: 0.004391661379486322\n",
      "epoch: 141, class_loss_value:0.007451660931110382, csis_loss_value: 0.019336819648742676, cycle_loss_value:0.01758401282131672\n",
      ">>>> testing_loss_value: 0.007451660931110382\n",
      "epoch: 142, class_loss_value:0.010403355583548546, csis_loss_value: 0.011536790058016777, cycle_loss_value:0.01898396760225296\n",
      ">>>> testing_loss_value: 0.010403355583548546\n",
      "epoch: 143, class_loss_value:0.011003242805600166, csis_loss_value: 0.017233259975910187, cycle_loss_value:0.017875969409942627\n",
      ">>>> testing_loss_value: 0.011003242805600166\n",
      "epoch: 144, class_loss_value:0.010288899764418602, csis_loss_value: 0.015481298789381981, cycle_loss_value:0.01780589111149311\n",
      ">>>> testing_loss_value: 0.010288899764418602\n",
      "epoch: 145, class_loss_value:0.009822583757340908, csis_loss_value: 0.01269792951643467, cycle_loss_value:0.01676081493496895\n",
      ">>>> testing_loss_value: 0.009822583757340908\n",
      "epoch: 146, class_loss_value:0.010088769719004631, csis_loss_value: 0.01233268715441227, cycle_loss_value:0.015603836625814438\n",
      ">>>> testing_loss_value: 0.010088769719004631\n",
      "epoch: 147, class_loss_value:0.01065331231802702, csis_loss_value: 0.010118166916072369, cycle_loss_value:0.01586410403251648\n",
      ">>>> testing_loss_value: 0.01065331231802702\n",
      "epoch: 148, class_loss_value:0.010635361075401306, csis_loss_value: 0.009031960740685463, cycle_loss_value:0.014402142725884914\n",
      ">>>> testing_loss_value: 0.010635361075401306\n",
      "epoch: 149, class_loss_value:0.010256629437208176, csis_loss_value: 0.009167476557195187, cycle_loss_value:0.014993375167250633\n",
      ">>>> testing_loss_value: 0.010256629437208176\n"
     ]
    }
   ],
   "source": [
    "source_dataloader, target_dataloader = CT_dataloader_train, dataloader_train\n",
    "test_dataloader = dataloader_test\n",
    "\n",
    "\n",
    "unetOptim = optim.Adam([\n",
    "                {'params': model_MRI.parameters(),},\n",
    "                {'params': model_CT.parameters()}, # 讓MRI有小一點的lr，ct用default lr\n",
    "                {'params': netG_CT2MRI.parameters()},\n",
    "                {'params': netG_MRI2CT.parameters()},\n",
    "            ], lr=1e-1)  # default lr\n",
    "\n",
    "EPOCHS = 150\n",
    "min_target_loss_value = 100\n",
    "source_domain_label = 1\n",
    "target_domain_label = 0\n",
    "\n",
    "for epoch in range(EPOCHS):  \n",
    "    class_loss_value = 0.0\n",
    "    csis_loss_value = 0.0\n",
    "    cycle_loss_value = 0.0\n",
    "    testing_loss_value = 0.0\n",
    "    for i, ((source_data, source_label), (target_data, target_label), (MRI_image)) in enumerate(zip(source_dataloader,\n",
    "                                                                                       target_dataloader, dataloader_nlb_train)):\n",
    "        source_data = source_data.to(device)\n",
    "        source_label = source_label.to(device)\n",
    "        target_data = target_data.to(device)\n",
    "        target_label = target_label.to(device)\n",
    "        MRI_image = MRI_image.to(device)\n",
    "        \n",
    "#         print(MRI_image.shape)\n",
    "        print(i, end='\\r')\n",
    "        \n",
    "        loss = train_disc(backward=False)\n",
    "        while loss.item()>0.5:\n",
    "            loss = train_disc()\n",
    "            \n",
    "        a, b, c = train_nonlabel_unet()\n",
    "        class_loss_value += a\n",
    "        csis_loss_value += b\n",
    "        cycle_loss_value += c\n",
    "        \n",
    "        if epoch%10==0:\n",
    "            a,b,c = train_label_unet()\n",
    "            class_loss_value *= 0.5\n",
    "            class_loss_value += a*0.5\n",
    "            csis_loss_value *= 0.5\n",
    "            csis_loss_value += b*0.5\n",
    "            cycle_loss_value *= 0.5\n",
    "            cycle_loss_value += c*0.5\n",
    "        \n",
    "        train_gen()\n",
    "        \n",
    "        class_loss_value /= 2\n",
    "        csis_loss_value /= 2\n",
    "        cycle_loss_value /= 2\n",
    "        \n",
    "        del source_data, source_label, target_data, a, b, c\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    class_loss_value /= (i+1)   \n",
    "    csis_loss_value /= (i+1)   \n",
    "    cycle_loss_value /= (i+1)\n",
    "    \n",
    "    testing_loss_value = class_loss_value\n",
    "#     for i2, (target_data, target_label) in enumerate(test_dataloader):\n",
    "#         target_data = target_data.to(device)\n",
    "#         target_label = target_label.to(device)\n",
    "        \n",
    "#         y_pred, _ = model_MRI(target_data)\n",
    "#         loss = class_criterion(y_pred, target_label)\n",
    "#         testing_loss_value += loss.item()\n",
    "        \n",
    "#         del target_data, target_label\n",
    "#         torch.cuda.empty_cache()\n",
    "#     testing_loss_value /= (i2+1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(f'epoch: {epoch}, class_loss_value:{class_loss_value}, csis_loss_value: {csis_loss_value}, cycle_loss_value:{cycle_loss_value}')\n",
    "    print(f'>>>> testing_loss_value: {testing_loss_value}')\n",
    "    if testing_loss_value < 0.9*min_target_loss_value:\n",
    "        min_target_loss_value = testing_loss_value\n",
    "        torch.save(netG_MRI2CT.state_dict(), f'{save_root}best_netG_MRI2CT.bin')\n",
    "        torch.save(netG_CT2MRI.state_dict(), f'{save_root}best_netG_CT2MRI.bin')\n",
    "        torch.save(model_CT.state_dict(), f'{save_root}best_model_CT.bin')\n",
    "        torch.save(model_MRI.state_dict(), f'{save_root}best_model_MRI.bin')\n",
    "    else:\n",
    "\n",
    "        torch.save(netG_MRI2CT.state_dict(), f'{save_root}netG_MRI2CT.bin')\n",
    "        torch.save(netG_CT2MRI.state_dict(), f'{save_root}netG_CT2MRI.bin')\n",
    "        torch.save(model_CT.state_dict(), f'{save_root}model_CT.bin')\n",
    "        torch.save(model_MRI.state_dict(), f'{save_root}model_MRI.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e32d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5236d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa9fb798",
   "metadata": {
    "id": "iGi60Uey47Mi"
   },
   "source": [
    "## 測試模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4eb52",
   "metadata": {
    "id": "sFhkIqgyaeyh"
   },
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f234a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jovyan/DA/DATA/TA_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/testset'\n",
    "\n",
    "dic = {}\n",
    "for a,b,c in os.walk(root, topdown=True):\n",
    "    if len(c)>0: # 當前目錄內包含檔案\n",
    "        if not a.__contains__('OutPhase'):\n",
    "            dic[a] = c\n",
    "dic.keys()\n",
    "dataset = {}\n",
    "# lis = ['CT', 'MRI']\n",
    "lis = ['DICOM_anon', 'Ground']\n",
    "for task in lis:\n",
    "    class_lis = []\n",
    "    for sub_folder in dic.keys():\n",
    "        if task in sub_folder.split('/'):\n",
    "            class_lis+=[sub_folder+'/'+filename for filename in dic[sub_folder]]\n",
    "    dataset[task] = class_lis\n",
    "    \n",
    "\n",
    "dataset['MRI_T2SPIR_test'] = sorted([i for i in dataset['DICOM_anon'] if 'T2SPIR' in i])\n",
    "dataset['MRI_T2SPIR_mask'] = sorted([i for i in dataset['Ground'] if 'T2SPIR' in i])\n",
    "assert len(dataset['MRI_T2SPIR_test'])==len(dataset['MRI_T2SPIR_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96d90e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MRI_test = list(zip(dataset['MRI_T2SPIR_test'], dataset['MRI_T2SPIR_mask']))\n",
    "\n",
    "# 建議同時間只有8個(256,256)的sample進行計算 (Total = BATCH_SIZE*MULTIPLE_BATCH)\n",
    "dataset_test = CustomImageDataset(MRI_data['test'], transform=target_transform, pseudo_label=False) \n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dacb0368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 256, 256]) torch.Size([8, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "a = iter(dataloader_test)\n",
    "bx, by = a.next()\n",
    "print(bx.shape, by.shape)\n",
    "del a, bx, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c23e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MRI =  Unet(out_sz=(HEIGHT, WIDTH), out_channels=ch_out, activation=None, multi_level=0).to(device)\n",
    "model_MRI.load_state_dict(torch.load(f'{save_root}model_MRI.bin')) \n",
    "class_criterion = DiceLoss()\n",
    "# class_criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title [Colab Form] 測試不同的dataloader \n",
    "#@markdown 可能的選擇包括 'dataloader_train', 'dataloader_test', 'CT_dataloader_test'\n",
    "# print(w.value)\n",
    "# test_dataloader = dic[w.value]\n",
    "import pandas as pd\n",
    "# test_dataloader = dataloader_test\n",
    "\n",
    "\n",
    "testing_loss = 0.0\n",
    "# 建立3個block\n",
    "\n",
    "with torch.no_grad(): # 避免torch計算gradient產生記憶體負擔\n",
    "    for i, data in enumerate(dataloader_test, 1): \n",
    "        torch.cuda.empty_cache()\n",
    "        image, mask = data\n",
    "\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        outputs, _ = model_MRI(image)\n",
    "\n",
    "        if True:\n",
    "            thres = 0.1\n",
    "            outputs[outputs<thres] = 0 \n",
    "            outputs[outputs!=0] = 1\n",
    "            outputs = outputs.long()\n",
    "\n",
    "#             outputs = mask # ground truth diceloss=0\n",
    "#         a = outputs\n",
    "        loss = class_criterion(outputs, mask)\n",
    "        testing_loss += loss\n",
    "    loss =  testing_loss/len(dataloader_test)\n",
    "    \n",
    "del outputs\n",
    "        \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    \"\"\"\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formatted\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "for file_list, dataloader in zip([MRI_test], [dataloader_test]):\n",
    "    test_list = [['-'.join([str(i[0].split('/')[idx]) for idx in [-4,-3,-1]])] for i in file_list]\n",
    "#     len(CT_test_list)\n",
    "\n",
    "    dataloader = iter(dataloader)\n",
    "    print(len(file_list))\n",
    "    i = 0\n",
    "    while 1:\n",
    "        try:\n",
    "            image, mask = dataloader.next()\n",
    "            image = image.to(device)\n",
    "#             mask = mask.to(device)\n",
    "            \n",
    "            outputs, _ = model_MRI(image)\n",
    "        \n",
    "\n",
    "            outputs[outputs<0.1] = 0.\n",
    "            outputs[outputs!=0] = 1.\n",
    "            outputs = outputs.detach().cpu()\n",
    "\n",
    "            for out in outputs:\n",
    "                test_list[i].append(mask2rle(out))\n",
    "\n",
    "                i += 1\n",
    "        except StopIteration:\n",
    "            print(i)\n",
    "            print('complete')\n",
    "            break\n",
    "    submission+=test_list\n",
    "    assert i==len(test_list)\n",
    "    \n",
    "pd.DataFrame(submission, columns=['filename', 'rle']).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dee0de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6d8f1fcd",
    "outputId": "b7b08657-c0f1-4ec6-c1c4-de5342a5c485"
   },
   "outputs": [],
   "source": [
    "# #@title [Colab Form] 測試不同的dataloader \n",
    "# #@markdown 可能的選擇包括 'dataloader_test', 'CT_dataloader_test'\n",
    "\n",
    "# test_dataloader = CT_dataloader_test\n",
    "\n",
    "\n",
    "# def test():\n",
    "#     with torch.no_grad(): # 避免torch計算gradient產生記憶體負擔\n",
    "#         for i, data in enumerate(test_dataloader, 1): \n",
    "#             torch.cuda.empty_cache()\n",
    "#             image, mask = data\n",
    "#             print(len(image))\n",
    "\n",
    "#             image = image.to(device)\n",
    "#             mask = mask.to(device)\n",
    "\n",
    "#             outputs, _ = model_MRI(image)\n",
    "# #             outputs = label_predictor(enc_ftrs)\n",
    "# #             print(outputs)\n",
    "#             # outputs = model(image)\n",
    "#             loss = class_criterion(outputs, mask)\n",
    "#             print(loss)\n",
    "\n",
    "#             '''\n",
    "#             設定閥值輸出:\n",
    "#             其實不設定也是可以輸出，不設定的情況會用灰階輸出，細節會更好一點\n",
    "#             '''\n",
    "#             if True: # 可以改成false，採灰階輸出， True的話會通過閾值輸出\n",
    "#                 # a = sum([i.max() for i in outputs])/len(outputs)\n",
    "#                 # b = sum([i.std() for i in outputs])/len(outputs)\n",
    "#                 # \n",
    "#                 # thres = a-1*b\n",
    "#                 thres = 0.1\n",
    "#                 outputs[outputs>=thres] = 1 \n",
    "#                 outputs[outputs<thres] = 0\n",
    "#                 outputs = outputs.long()\n",
    "\n",
    "#             print(image[:,0,:,:].shape)\n",
    "#             print(mask.shape)\n",
    "#             print(outputs.shape)\n",
    "\n",
    "#             img_process = lambda image:image[:,0,:,:].detach().cpu().numpy()\n",
    "#             mask_process = lambda mask:mask.squeeze(1).detach().cpu().numpy()\n",
    "\n",
    "#             for x, m, p in [*zip(img_process(image), mask_process(mask), mask_process(outputs))]:\n",
    "#                   # show_image_mask(x, m, p)\n",
    "#                 show_image(x,m,p)\n",
    "\n",
    "#             del image, mask, outputs, data, loss # 把記憶體清掉\n",
    "#             torch.cuda.empty_cache()\n",
    "#             break\n",
    "  \n",
    "    \n",
    "# tmp = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42d333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CHAOS_CT_MRI_Unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
