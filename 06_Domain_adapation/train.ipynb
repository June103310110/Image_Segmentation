{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b95d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 # 8 for 256x256/ 16 for 128x128\n",
    "WIDTH = 256\n",
    "HEIGHT = 256 \n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787503c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from unet import UNet\n",
    "from dataset import getAllDataPath, CTMRI_ImageDataset\n",
    "from criterion import DiceLoss, FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51e73db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_data train (2299, 2)\n",
      "CT_data test (575, 2)\n",
      "MRI_data train (123, 2)\n",
      "MRI_data test (31, 2)\n",
      "MRI_imgOnly_data train (309,)\n"
     ]
    }
   ],
   "source": [
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/CT/'\n",
    "CT_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_Label/'\n",
    "MRI_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_nonLabel/'\n",
    "MRI_imgOnly_data = getAllDataPath(root, imgOnly=True)\n",
    "\n",
    "for data in ['CT_data', 'MRI_data', 'MRI_imgOnly_data']:\n",
    "    i = eval(data)\n",
    "    for k in i.keys():\n",
    "        print(data,k, np.shape(i[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6540baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
    "\n",
    "transform = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(brightness_limit=[-0.05, 0.05], p=0.2),\n",
    "#     A.Rotate((-30, 30), interpolation=0), \n",
    "#     A.RandomContrast(limit=0.2, p=1), \n",
    "\n",
    "#     A.Normalize(p=1, mean=(0.485), std=(0.229)),\n",
    "#     A.ToFloat(always_apply=True),\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])\n",
    "\n",
    "target_transform = A.Compose([\n",
    "#     A.Normalize(p=1, mean=(0.485), std=(0.229)),                         \n",
    "#     A.ToFloat(always_apply=True),\n",
    "    A.Resize(WIDTH, HEIGHT),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013c3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CTMRI_ImageDataset(MRI_data['train'], transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "dataset_test = CTMRI_ImageDataset(MRI_data['test'], transform=target_transform) # **如果要正式使用要記得把這裡換成X_test\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "CT_dataset_train = CTMRI_ImageDataset(CT_data['train'], transform=transform)\n",
    "CT_dataloader_train = torch.utils.data.DataLoader(CT_dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "CT_dataset_test = CTMRI_ImageDataset(CT_data['test'], transform=target_transform)\n",
    "CT_dataloader_test = torch.utils.data.DataLoader(CT_dataset_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b9d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 8\n",
      "---\n",
      "dataloader_train 15\n",
      "[torch.Size([8, 1, 256, 256]), torch.Size([8, 1, 256, 256])]\n",
      "[(tensor(1.), tensor(0.)), (tensor(1.), tensor(0.))]\n",
      "---\n",
      "dataloader_test 4\n",
      "[torch.Size([8, 1, 256, 256]), torch.Size([8, 1, 256, 256])]\n",
      "[(tensor(1.), tensor(0.)), (tensor(1.), tensor(0.))]\n",
      "---\n",
      "CT_dataloader_train 287\n",
      "[torch.Size([8, 1, 256, 256]), torch.Size([8, 1, 256, 256])]\n",
      "[(tensor(0.9974), tensor(0.)), (tensor(1.), tensor(0.))]\n",
      "---\n",
      "CT_dataloader_test 72\n",
      "[torch.Size([8, 1, 256, 256]), torch.Size([8, 1, 256, 256])]\n",
      "[(tensor(1.), tensor(0.)), (tensor(1.), tensor(0.))]\n"
     ]
    }
   ],
   "source": [
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "for i in ['dataloader_train', 'dataloader_test', 'CT_dataloader_train', 'CT_dataloader_test']:\n",
    "    loader = eval(i)\n",
    "    print('---')\n",
    "    print(i, loader.__len__())\n",
    "    print([i.shape for i in iter(loader).next()])\n",
    "    print([(i.max(), i.min()) for i in iter(loader).next()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637ce9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_label_unet(backward=True):\n",
    "    # part1 \n",
    "    for model in [model_MRI]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    mriOptim.zero_grad()\n",
    "    'y pred'\n",
    "    MRI_pred = model_MRI(target_data)\n",
    "\n",
    "    '''\n",
    "    class loss\n",
    "    '''\n",
    "#     MRI_class_loss = class_criterion(MRI_pred, target_label)\n",
    "#     print(MRI_pred.shape, target_label.shape)\n",
    "#     print(MRI_pred.min())\n",
    "#     print(target_label.unique())\n",
    "    MRI_class_loss = sigmoid_focal_loss(MRI_pred, target_label, alpha = 0.25, gamma = 2, reduction = 'mean')\n",
    "    \n",
    "#     if MRI_pred.data.size()[1]>1:\n",
    "#         MRI_pred = F.softmax(MRI_pred, dim=1)\n",
    "#         MRI_pred = torch.argmax(MRI_pred, dim=1)\n",
    "    \n",
    "#     MRI_dice_loss = dice_criterion(MRI_pred, target_label)\n",
    "#         MRI_dice_loss += dice_criterion(MRI_pred[:,0,:,:], target_label)\n",
    "#         MRI_dice_loss /= 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     lamb = WIDTH*HEIGHT/MRI_pred.data.size()[0]\n",
    "    lamb = 0\n",
    "    MRI_dice_loss = 0.0\n",
    "#     MRI_loss = MRI_class_loss# + lamb*MRI_dice_loss\n",
    "  \n",
    "    if backward:\n",
    "        mriOptim.zero_grad()\n",
    "        MRI_class_loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model_MRI.parameters(), max_norm = 10)\n",
    "        mriOptim.step()\n",
    "        \n",
    "    del MRI_pred\n",
    "    \n",
    "    return MRI_class_loss, lamb*MRI_dice_loss, 0#, csis_loss/2, cycle_loss/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6195cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import sigmoid_focal_loss\n",
    "save_root = 'saved/0420-1/'\n",
    "dice_criterion = DiceLoss()\n",
    "class_criterion = FocalLoss(gamma=2, alpha=[0.25, 0.75], size_average=False)\n",
    "# class_criterion = sigmoid_focal_loss#(gamma=2, alpha=[0.25, 0.75])\n",
    "# domain_criterion = nn.BCEWithLogitsLoss()\n",
    "# consist_criterion = nn.L1Loss()\n",
    "# device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b31263",
   "metadata": {},
   "source": [
    "- 測試看看torch vision focalloss bce (輸出1ch)\n",
    "- softmax head\n",
    "- sigmoid head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c82e994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, class_loss_value: 0.036406, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 1, class_loss_value: 0.010669, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 2, class_loss_value: 0.009587, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 3, class_loss_value: 0.009402, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 4, class_loss_value: 0.009263, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 5, class_loss_value: 0.008572, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 6, class_loss_value: 0.008180, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 7, class_loss_value: 0.007729, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 8, class_loss_value: 0.007124, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 9, class_loss_value: 0.007026, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 10, class_loss_value: 0.006542, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 11, class_loss_value: 0.006619, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 12, class_loss_value: 0.005637, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 13, class_loss_value: 0.005286, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 14, class_loss_value: 0.005708, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 15, class_loss_value: 0.005104, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 16, class_loss_value: 0.004615, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 17, class_loss_value: 0.003782, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 18, class_loss_value: 0.004855, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 19, class_loss_value: 0.005198, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 20, class_loss_value: 0.004384, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 21, class_loss_value: 0.004212, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 22, class_loss_value: 0.004043, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 23, class_loss_value: 0.003244, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 24, class_loss_value: 0.003417, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 25, class_loss_value: 0.003579, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 26, class_loss_value: 0.002817, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 27, class_loss_value: 0.004760, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 28, class_loss_value: 0.003317, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 29, class_loss_value: 0.003567, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 30, class_loss_value: 0.002768, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 31, class_loss_value: 0.002596, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 32, class_loss_value: 0.002994, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 33, class_loss_value: 0.002044, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 34, class_loss_value: 0.002308, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 35, class_loss_value: 0.002550, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 36, class_loss_value: 0.002479, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 37, class_loss_value: 0.002360, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 38, class_loss_value: 0.002381, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 39, class_loss_value: 0.001878, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 40, class_loss_value: 0.002005, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 41, class_loss_value: 0.002062, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 42, class_loss_value: 0.001699, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 43, class_loss_value: 0.001882, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 44, class_loss_value: 0.001609, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 45, class_loss_value: 0.001446, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 46, class_loss_value: 0.001512, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 47, class_loss_value: 0.001409, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 48, class_loss_value: 0.001176, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 49, class_loss_value: 0.001189, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 50, class_loss_value: 0.001233, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 51, class_loss_value: 0.001130, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 52, class_loss_value: 0.001150, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 53, class_loss_value: 0.001161, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 54, class_loss_value: 0.001135, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 55, class_loss_value: 0.001058, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 56, class_loss_value: 0.001106, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 57, class_loss_value: 0.001141, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 58, class_loss_value: 0.001268, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 59, class_loss_value: 0.001161, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 60, class_loss_value: 0.001727, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 61, class_loss_value: 0.001388, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 62, class_loss_value: 0.001326, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 63, class_loss_value: 0.001288, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 64, class_loss_value: 0.001186, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 65, class_loss_value: 0.001372, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 66, class_loss_value: 0.001264, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 67, class_loss_value: 0.001251, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 68, class_loss_value: 0.001087, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 69, class_loss_value: 0.001023, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 70, class_loss_value: 0.000924, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 71, class_loss_value: 0.000883, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 72, class_loss_value: 0.000875, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 73, class_loss_value: 0.000950, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 74, class_loss_value: 0.000885, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 75, class_loss_value: 0.000960, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 76, class_loss_value: 0.001087, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 77, class_loss_value: 0.001003, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 78, class_loss_value: 0.000872, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 79, class_loss_value: 0.000929, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 80, class_loss_value: 0.000860, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 81, class_loss_value: 0.000798, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 82, class_loss_value: 0.000836, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 83, class_loss_value: 0.000787, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 84, class_loss_value: 0.000851, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 85, class_loss_value: 0.000885, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 86, class_loss_value: 0.000923, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 87, class_loss_value: 0.000825, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 88, class_loss_value: 0.000791, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 89, class_loss_value: 0.000737, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 90, class_loss_value: 0.000650, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 91, class_loss_value: 0.000653, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 92, class_loss_value: 0.000782, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 93, class_loss_value: 0.000781, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 94, class_loss_value: 0.000677, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 95, class_loss_value: 0.000671, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 96, class_loss_value: 0.000721, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 97, class_loss_value: 0.000901, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 98, class_loss_value: 0.001546, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 99, class_loss_value: 0.001703, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 100, class_loss_value: 0.001544, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 101, class_loss_value: 0.001485, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 102, class_loss_value: 0.002029, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 103, class_loss_value: 0.001578, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 104, class_loss_value: 0.001178, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 105, class_loss_value: 0.000986, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 106, class_loss_value: 0.000876, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 107, class_loss_value: 0.000823, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 108, class_loss_value: 0.000748, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 109, class_loss_value: 0.000774, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 110, class_loss_value: 0.000708, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 111, class_loss_value: 0.000695, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 112, class_loss_value: 0.000675, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 113, class_loss_value: 0.000672, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 114, class_loss_value: 0.000665, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 115, class_loss_value: 0.000629, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 116, class_loss_value: 0.000628, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 117, class_loss_value: 0.000621, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 118, class_loss_value: 0.000635, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 119, class_loss_value: 0.000655, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 120, class_loss_value: 0.000592, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 121, class_loss_value: 0.000610, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 122, class_loss_value: 0.000611, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 123, class_loss_value: 0.000595, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 124, class_loss_value: 0.000649, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 125, class_loss_value: 0.000565, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 126, class_loss_value: 0.000621, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 127, class_loss_value: 0.000632, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 128, class_loss_value: 0.000605, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 129, class_loss_value: 0.000583, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 130, class_loss_value: 0.000570, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 131, class_loss_value: 0.000535, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 132, class_loss_value: 0.000583, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 133, class_loss_value: 0.000589, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 134, class_loss_value: 0.000640, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 135, class_loss_value: 0.000620, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 136, class_loss_value: 0.000552, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 137, class_loss_value: 0.000541, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 138, class_loss_value: 0.000556, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 139, class_loss_value: 0.000507, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 140, class_loss_value: 0.000481, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 141, class_loss_value: 0.000514, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 142, class_loss_value: 0.000536, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 143, class_loss_value: 0.000552, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 144, class_loss_value: 0.000542, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 145, class_loss_value: 0.000506, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 146, class_loss_value: 0.000528, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 147, class_loss_value: 0.000514, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 148, class_loss_value: 0.000523, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 149, class_loss_value: 0.000511, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 150, class_loss_value: 0.000492, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 151, class_loss_value: 0.000460, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 152, class_loss_value: 0.000487, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 153, class_loss_value: 0.000479, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 154, class_loss_value: 0.000501, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 155, class_loss_value: 0.000507, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 156, class_loss_value: 0.000506, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 157, class_loss_value: 0.000482, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 158, class_loss_value: 0.000477, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 159, class_loss_value: 0.000459, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 160, class_loss_value: 0.000522, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 161, class_loss_value: 0.000489, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 162, class_loss_value: 0.000498, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 163, class_loss_value: 0.000536, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 164, class_loss_value: 0.000563, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 165, class_loss_value: 0.000508, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 166, class_loss_value: 0.000459, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 167, class_loss_value: 0.000464, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 168, class_loss_value: 0.000447, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 169, class_loss_value: 0.000440, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 170, class_loss_value: 0.000446, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 171, class_loss_value: 0.000456, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 172, class_loss_value: 0.000472, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 173, class_loss_value: 0.000581, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 174, class_loss_value: 0.000742, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 175, class_loss_value: 0.000847, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 176, class_loss_value: 0.000741, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 177, class_loss_value: 0.000676, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 178, class_loss_value: 0.000566, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 179, class_loss_value: 0.000528, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 180, class_loss_value: 0.000515, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 181, class_loss_value: 0.000495, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 182, class_loss_value: 0.000492, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 183, class_loss_value: 0.000490, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 184, class_loss_value: 0.000467, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 185, class_loss_value: 0.000408, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 186, class_loss_value: 0.000408, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 187, class_loss_value: 0.000414, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 188, class_loss_value: 0.000410, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 189, class_loss_value: 0.000428, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 190, class_loss_value: 0.000434, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 191, class_loss_value: 0.000446, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 192, class_loss_value: 0.000403, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 193, class_loss_value: 0.000428, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 194, class_loss_value: 0.000399, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 195, class_loss_value: 0.000406, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 196, class_loss_value: 0.000442, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 197, class_loss_value: 0.000415, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 198, class_loss_value: 0.000381, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 199, class_loss_value: 0.000392, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 200, class_loss_value: 0.000381, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 201, class_loss_value: 0.000372, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 202, class_loss_value: 0.000361, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 203, class_loss_value: 0.000373, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 204, class_loss_value: 0.000348, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 205, class_loss_value: 0.000358, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 206, class_loss_value: 0.000380, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 207, class_loss_value: 0.000426, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 208, class_loss_value: 0.000448, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 209, class_loss_value: 0.000422, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 210, class_loss_value: 0.000444, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 211, class_loss_value: 0.000396, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 212, class_loss_value: 0.000385, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 213, class_loss_value: 0.000448, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 214, class_loss_value: 0.000420, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 215, class_loss_value: 0.000434, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 216, class_loss_value: 0.000479, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 217, class_loss_value: 0.000416, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 218, class_loss_value: 0.000394, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 219, class_loss_value: 0.000375, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 220, class_loss_value: 0.000352, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 221, class_loss_value: 0.000344, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "save best model\n",
      "epoch: 222, class_loss_value: 0.000361, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 223, class_loss_value: 0.000354, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 224, class_loss_value: 0.000356, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 225, class_loss_value: 0.000347, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 226, class_loss_value: 0.000375, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 227, class_loss_value: 0.000371, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 228, class_loss_value: 0.000390, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 229, class_loss_value: 0.000370, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 230, class_loss_value: 0.000392, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 231, class_loss_value: 0.002224, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 232, class_loss_value: 0.009248, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 233, class_loss_value: 0.006651, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 234, class_loss_value: 0.005340, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 235, class_loss_value: 0.004533, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 236, class_loss_value: 0.004420, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 237, class_loss_value: 0.003373, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 238, class_loss_value: 0.003379, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 239, class_loss_value: 0.004147, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 240, class_loss_value: 0.002812, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 241, class_loss_value: 0.002210, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 242, class_loss_value: 0.003437, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 243, class_loss_value: 0.003045, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 244, class_loss_value: 0.003267, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 245, class_loss_value: 0.004784, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 246, class_loss_value: 0.002860, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 247, class_loss_value: 0.002097, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 248, class_loss_value: 0.001907, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 249, class_loss_value: 0.001731, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 250, class_loss_value: 0.001501, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 251, class_loss_value: 0.001476, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 252, class_loss_value: 0.001573, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 253, class_loss_value: 0.001740, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 254, class_loss_value: 0.001314, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 255, class_loss_value: 0.001212, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 256, class_loss_value: 0.001151, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 257, class_loss_value: 0.001060, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 258, class_loss_value: 0.001062, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 259, class_loss_value: 0.001025, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 260, class_loss_value: 0.001109, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 261, class_loss_value: 0.001139, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 262, class_loss_value: 0.001140, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 263, class_loss_value: 0.001142, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 264, class_loss_value: 0.001046, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 265, class_loss_value: 0.000942, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 266, class_loss_value: 0.000885, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 267, class_loss_value: 0.000897, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 268, class_loss_value: 0.000868, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 269, class_loss_value: 0.000894, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 270, class_loss_value: 0.000803, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 271, class_loss_value: 0.000774, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 272, class_loss_value: 0.000749, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 273, class_loss_value: 0.000721, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 274, class_loss_value: 0.000935, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 275, class_loss_value: 0.000958, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 276, class_loss_value: 0.000852, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 277, class_loss_value: 0.000857, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 278, class_loss_value: 0.000785, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 279, class_loss_value: 0.000770, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 280, class_loss_value: 0.000711, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 281, class_loss_value: 0.000720, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 282, class_loss_value: 0.000682, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 283, class_loss_value: 0.000625, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 284, class_loss_value: 0.000609, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 285, class_loss_value: 0.000675, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 286, class_loss_value: 0.000738, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 287, class_loss_value: 0.000756, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 288, class_loss_value: 0.000689, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 289, class_loss_value: 0.000616, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 290, class_loss_value: 0.000568, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 291, class_loss_value: 0.000577, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 292, class_loss_value: 0.000579, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 293, class_loss_value: 0.000590, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 294, class_loss_value: 0.000590, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 295, class_loss_value: 0.000586, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 296, class_loss_value: 0.000588, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 297, class_loss_value: 0.000516, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 298, class_loss_value: 0.000505, dice_loss_value:  0.000000, cycle_loss_value:0.0\n",
      "epoch: 299, class_loss_value: 0.000484, dice_loss_value:  0.000000, cycle_loss_value:0.0\n"
     ]
    }
   ],
   "source": [
    "source_dataloader, target_dataloader = CT_dataloader_train, dataloader_train\n",
    "test_dataloader = dataloader_test # CT_dataloader_test, dataloader_test\n",
    "\n",
    "model_MRI =  UNet(out_sz=(HEIGHT, WIDTH), num_class=1, enc_chs=(1,64,128,256,512,1024),\n",
    "                  activation=None, # nn.Softmax(dim=1)\n",
    "                 ).to(device) \n",
    "mriOptim = optim.Adam(model_MRI.parameters(), lr=1e-1)\n",
    "\n",
    "EPOCHS = 300\n",
    "min_target_loss_value = 100\n",
    "source_domain_label = 1\n",
    "target_domain_label = 0\n",
    "\n",
    "for epoch in range(EPOCHS):  \n",
    "    class_loss_value = 0.0\n",
    "    dice_loss_value = 0.0\n",
    "    cycle_loss_value = 0.0\n",
    "    testing_loss_value = 0.0\n",
    "    warmup = 5\n",
    "    \n",
    "\n",
    "    \n",
    "    for i, ((source_data, source_label), (target_data, target_label)) in enumerate(zip(source_dataloader,\n",
    "                                                                                       target_dataloader)):\n",
    "#         source_data = source_data.to(device)\n",
    "#         source_label = source_label.to(device)\n",
    "        target_data = target_data.to(device, dtype = torch.float32)\n",
    "        target_label = target_label.to(device, dtype = torch.float32)\n",
    "\n",
    "        a,b,c = train_label_unet()\n",
    "        class_loss_value += a\n",
    "        dice_loss_value += b\n",
    "        \n",
    "        print(i, end='\\r')\n",
    "        del source_data, source_label, target_data, a, b, c\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    class_loss_value /= (i+1)   \n",
    "    dice_loss_value /= (i+1)   \n",
    "\n",
    "    testing_loss_value = class_loss_value\n",
    "\n",
    "    print(f'epoch: {epoch}, class_loss_value:{class_loss_value:9.6f}, dice_loss_value: {dice_loss_value:9.6f}, cycle_loss_value:{cycle_loss_value}')\n",
    "    if testing_loss_value < min_target_loss_value:\n",
    "        min_target_loss_value = testing_loss_value\n",
    "        print('save best model')\n",
    "        torch.save(model_MRI.state_dict(), f'{save_root}best_model_MRI.bin')\n",
    "    else:\n",
    "        if epoch%20==0:\n",
    "            torch.save(model_MRI.state_dict(), f'{save_root}E{epoch}_model_MRI.bin')\n",
    "        torch.save(model_MRI.state_dict(), f'{save_root}model_MRI.bin')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8156ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loader \n",
    "# test_dataloader = dataloader_test\n",
    "# device = 'cpu'\n",
    "# dice_loss_value = 0.0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, (target_data, target_label) in enumerate(test_dataloader):\n",
    "#         #         source_data = source_data.to(device)\n",
    "#         #         source_label = source_label.to(device)\n",
    "#         target_data = target_data.to(device)\n",
    "#         target_label = target_label.to(device)\n",
    "#         model_MRI = model_MRI.to(device)\n",
    "\n",
    "#         outputs = model_MRI(target_data)\n",
    "# #         outputs = F.sigmoid(outputs)\n",
    "\n",
    "#         if True:\n",
    "#             threshold = 0\n",
    "#             outputs[outputs>threshold] = 1.\n",
    "#             outputs[outputs!=1.] = 0.\n",
    "#             outputs = outputs#.detach().cpu().numpy()\n",
    "#     #     print(outputs.max())\n",
    "#     #     outputs = F.log_softmax(outputs, dim=1)*-1\n",
    "#     #     for o, t in zip(outputs, target_label):\n",
    "#     #         z = o[1]\n",
    "#     #         t = t.squeeze(0)\n",
    "#     # #         print(z.shape, t.shape)\n",
    "#     #         print('ch1 t=1', z[t==1])\n",
    "#     #         print('ch1 t=0', z[t==0])\n",
    "#     #         z = o[1]\n",
    "#     #         t = t.squeeze(0)\n",
    "#     #         print('ch1', z[t==1])\n",
    "#     #     outputs = F.softmax(outputs, dim=1)\n",
    "#     #     outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "#         loss = DiceLoss()(outputs, target_label)\n",
    "#         dice_loss_value += loss\n",
    "\n",
    "#         print(i, end='\\r')\n",
    "#         del target_data, target_label\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         break\n",
    "#     dice_loss_value /= (i+1)   \n",
    "\n",
    "# dice_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d700ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train.ipynb to python\n",
      "[NbConvertApp] Writing 8260 bytes to train.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    if get_ipython().__class__.__name__=='ZMQInteractiveShell':\n",
    "        os.system('jupyter nbconvert train.ipynb --to python')\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff69e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
