{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97686912",
   "metadata": {
    "id": "Egx-Nw5a6qiy"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 8 # 8 for 256x256/ 16 for 128x128\n",
    "# NUM_LABELS = 1\n",
    "# WIDTH = 256\n",
    "# HEIGHT = 256 \n",
    "# MULTI_CHANNELS = False\n",
    "# device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67daee85",
   "metadata": {
    "id": "21f15bc3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e755452",
   "metadata": {
    "id": "7f10bb28"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch # 1.9\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import torch.optim as optim\n",
    "# import time\n",
    "# import ipywidgets as widgets\n",
    "# import pickle\n",
    "\n",
    "# # 從repo裡面導入套件\n",
    "# from utils import show_image_mask, mask_CutMix#, patience\n",
    "# from unet import UNet\n",
    "\n",
    "\n",
    "# 導入dicom套件\n",
    "from pydicom import dcmread\n",
    "from pydicom.data import get_testdata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71feee97",
   "metadata": {
    "id": "H4LL5R2Hqxgy"
   },
   "outputs": [],
   "source": [
    "def show_image(*img_):\n",
    "    for i in img_:\n",
    "        assert i.__class__.__name__ == 'ndarray', 'imput data type should be ndarray'\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for i, img in enumerate(list(img_), 1):\n",
    "        plt.subplot(1,len(img_),i)\n",
    "\n",
    "        if len(np.shape(img)) == 2 or np.shape(img)[-1] == 1:\n",
    "            plt.imshow(img, cmap='gray')\n",
    "        elif len(np.shape(img)) == 3:\n",
    "            plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6b0e6",
   "metadata": {
    "id": "d3acdca8"
   },
   "source": [
    "### Build torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8417602b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def getImg(path):\n",
    "#     if path.__contains__('.dcm'):  \n",
    "#       # pydcm read image\n",
    "#         ds = dcmread(path)\n",
    "#         file = ds.pixel_array\n",
    "#         file = file.astype('uint8') # 調整格式以配合albumentation套件需求\n",
    "#     elif path.__contains__('.png'):\n",
    "#         file = cv2.imread(path)[...,0]\n",
    "#         file = file.astype('float32') # 調整格式以配合albumentation套件需求\n",
    "        \n",
    "#         if 'MRI' in path:\n",
    "#             file[file!=63] = 0\n",
    "#             file[file!=0] = 1\n",
    "#         elif 'CT' in path:\n",
    "#             file /= 255\n",
    "#         else:\n",
    "#             raise ValueError('Non-support dtype')\n",
    "#     else:\n",
    "#         raise ValueError(f'img format: {path} unknown')\n",
    "#     return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f00cfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllDataPath(dir_path, imgOnly=False, test_split_size=None):\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(dir_path)):\n",
    "        for file in sorted(files):\n",
    "            if '.dcm' in file:\n",
    "                images.append(os.path.join(root, file))\n",
    "            elif '.png' in file:\n",
    "                labels.append(os.path.join(root, file))\n",
    "    if imgOnly:\n",
    "        data_list = images\n",
    "    else:\n",
    "        data_list = list(zip(images, labels))\n",
    "\n",
    "    if test_split_size:\n",
    "        assert type(test_split_size)==float, 'set float to split test set size'\n",
    "        train, test = train_test_split(data_list,\n",
    "                         test_size = test_split_size)\n",
    "        return {'train':train, 'test':test}\n",
    "    else:\n",
    "        return {'train':data_list}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373c225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT_data train (2299, 2)\n",
      "CT_data test (575, 2)\n",
      "MRI_data train (123, 2)\n",
      "MRI_data test (31, 2)\n",
      "MRI_imgOnly_data train (309,)\n"
     ]
    }
   ],
   "source": [
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/CT/'\n",
    "CT_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_Label/'\n",
    "MRI_data = getAllDataPath(root, test_split_size=0.2)\n",
    "root = '/home/jovyan/DA/DATA/ST_data/CHAOS_AIAdatasets/2_Domain_Adaptation_dataset/MRI/MRI_nonLabel/'\n",
    "MRI_imgOnly_data = getAllDataPath(root, imgOnly=True)\n",
    "\n",
    "for data in ['CT_data', 'MRI_data', 'MRI_imgOnly_data']:\n",
    "    i = eval(data)\n",
    "    for k in i.keys():\n",
    "        print(data,k, np.shape(i[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe20450",
   "metadata": {
    "id": "53d773d0"
   },
   "outputs": [],
   "source": [
    "#  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "import cv2\n",
    "class CTMRI_ImageDataset(Dataset):\n",
    "    def __init__(self, imgs_anno_path_list,\n",
    "                 #dtype, \n",
    "#                  dir_path,\n",
    "                 transform=None):\n",
    "        self.imgs_anno_path_list = imgs_anno_path_list\n",
    "        self.transform = transform\n",
    "\n",
    "#   \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_anno_path_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # now = time.time()\n",
    "        imgOnly = False\n",
    "        img_anno_path = self.imgs_anno_path_list[idx]\n",
    "\n",
    "        if type(img_anno_path)==tuple:\n",
    "#             img_anno_path = [i for i in img_anno_path]\n",
    "            image = self.getImg(img_anno_path[0])\n",
    "            mask = self.getImg(img_anno_path[1])\n",
    "        else:\n",
    "            image = self.getImg(img_anno_path)\n",
    "            imgOnly = True\n",
    "    \n",
    "        \n",
    "        if imgOnly:\n",
    "            if self.transform:        \n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "#             image = np.concatenate((image, image, image), axis=0)\n",
    "            image = torch.Tensor(image)\n",
    "            return image\n",
    "        else:\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image, mask=mask)\n",
    "                image = transformed['image']\n",
    "                mask = transformed['mask']\n",
    "#                 print('2', image.max())\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "#             image = np.concatenate((image, image, image), axis=0)\n",
    "            image = torch.Tensor(image)\n",
    "\n",
    "            mask = torch.Tensor(mask) \n",
    "            mask = mask.unsqueeze(0)\n",
    "            return image, mask\n",
    "    \n",
    "    def getImg(self, path):\n",
    "        if path.__contains__('.dcm'):  \n",
    "          # pydcm read image\n",
    "            ds = dcmread(path)\n",
    "            file = ds.pixel_array\n",
    "            # image process\n",
    "            file = cv2.medianBlur(file, 5)\n",
    "            file = cv2.normalize(file, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "#             print(np.histogram(file, bins=10))\n",
    "#             file = file.astype('float32') # 調整格式以配合albumentation套件需求\n",
    "        elif path.__contains__('.png'):\n",
    "            file = cv2.imread(path)[...,0]\n",
    "            file = file.astype('float32') # 調整格式以配合albumentation套件需求\n",
    "\n",
    "            if 'MRI' in path:\n",
    "#                 pass\n",
    "                file[file!=63] = 0\n",
    "                file[file!=0] = 1\n",
    "            elif 'CT' in path:\n",
    "                file /= 255\n",
    "            else:\n",
    "                raise ValueError('Non-support dtype')\n",
    "        else:\n",
    "            raise ValueError(f'img format: {path} unknown')\n",
    "        return file\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbfe7b",
   "metadata": {
    "id": "X-W1854CunWa"
   },
   "source": [
    "#### 使用albumentations進行資料擴增"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "429b315a",
   "metadata": {
    "id": "77bc4194"
   },
   "outputs": [],
   "source": [
    "# # https://albumentations.ai/docs/getting_started/mask_augmentation/\n",
    "\n",
    "# BATCH_SIZE = 8\n",
    "# WIDTH, HEIGHT = (256,256)\n",
    "\n",
    "# transform = A.Compose([\n",
    "# #     A.HorizontalFlip(p=0.5),\n",
    "# #     A.RandomBrightnessContrast(brightness_limit=[-0.05, 0.05], p=0.2),\n",
    "# #     A.Rotate((-30, 30), interpolation=0), \n",
    "# #     A.RandomContrast(limit=0.2, p=1), \n",
    "# #     A.MedianBlur(always_apply=True, blur_limit=(3, 5)),\n",
    "\n",
    "# #     A.Normalize(p=1, mean=(0.485), std=(0.229)),\n",
    "# #     A.ToFloat(always_apply=True),\n",
    "#     A.Resize(WIDTH, HEIGHT),\n",
    "# ])\n",
    "\n",
    "# target_transform = A.Compose([\n",
    "# #     A.Normalize(p=1, mean=(0.485), std=(0.229)),     \n",
    "# #     A.MedianBlur(always_apply=True, blur_limit=(3, 5)),\n",
    "# #     A.ToFloat(always_apply=True),\n",
    "#     A.Resize(WIDTH, HEIGHT),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ea57c",
   "metadata": {
    "id": "Qq7TQNmwzudB"
   },
   "source": [
    "### 建立DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59537d00",
   "metadata": {
    "id": "d10b3fe6"
   },
   "outputs": [],
   "source": [
    "if '__main__' == __name__:\n",
    "# 建議同時間只有8個(256,256)的sample進行計算 (Total = BATCH_SIZE*MULTIPLE_BATCH)\n",
    "\n",
    "    dataset_train = CTMRI_ImageDataset(MRI_data['train'], transform=transform)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    dataset_test = CTMRI_ImageDataset(MRI_data['test'], transform=target_transform) # **如果要正式使用要記得把這裡換成X_test\n",
    "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    CT_dataset_train = CTMRI_ImageDataset(CT_data['train'], transform=transform)\n",
    "    CT_dataloader_train = torch.utils.data.DataLoader(CT_dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    CT_dataset_test = CTMRI_ImageDataset(CT_data['test'], transform=target_transform)\n",
    "    CT_dataloader_test = torch.utils.data.DataLoader(CT_dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "#     a = iter(dataloader_train)\n",
    "#     x, y = a.next()\n",
    "    all_y = torch.Tensor([])\n",
    "    all_x = 0\n",
    "    for batch in CT_dataset_train:\n",
    "        x, y = batch\n",
    "        all_y = torch.cat([all_y, y])\n",
    "        \n",
    "#         all_x += len(x[x>1])\n",
    "#         assert len(x[x>1])>0\n",
    "#         print(x.max().item())\n",
    "#         all_x.append(x.max().item())\n",
    "#     print(all_y.unique())\n",
    "#     print(all_x)\n",
    "#     print(max(all_x))\n",
    "\n",
    "# MRI_imgOnly_dataset_train = ImageOnly_Dataset(MRI_imgOnly_data['train'], transform=transform)\n",
    "# MRI_image_dataloader = torch.utils.data.DataLoader(MRI_imgOnly_dataset_train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4f96e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook dataset.ipynb to python\n",
      "[NbConvertApp] Writing 8677 bytes to dataset.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if '__main__' == __name__:\n",
    "    try:\n",
    "        if get_ipython().__class__.__name__=='ZMQInteractiveShell':\n",
    "            os.system('jupyter nbconvert dataset.ipynb --to python')\n",
    "    except NameError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225e4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CHAOS_CT_MRI_Unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
